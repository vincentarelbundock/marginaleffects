---
title: "Hypothesis Tests"
---

```{r, include = FALSE}
options(width = 1000)
## this vignette is in .Rbuildignore because lme4 is not available on old CRAN
## test machines.

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 9,
  fig.asp = .4,
  out.width = "100%",
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

This vignette introduces the `hypotheses()` function, and the `hypothesis` argument of the `comparisons()`, `slopes()`, and `predictions()` function. These features allow users to conduct linear and non-linear hypothesis tests and to compute custom contrasts (linear combinations) between parameters.

## Null hypothesis

The simplest way to modify a hypothesis test is to change the null hypothesis. By default, all functions in the `marginaleffects` package assume that the null is 0. This can be changed by changing the `hypothesis` argument. 

For example, consider a logistic regression model:

```{r}
library(marginaleffects)
mod <- glm(am ~ hp + drat, data = mtcars, family = binomial)
```

We can compute the predicted outcome for a hypothetical unit where all regressors are fixed to their sample means:

```{r}
predictions(mod, newdata = "mean")
```

The Z statistic and p value reported above assume that the null hypothesis equals zero. We can change the null with the `hypothesis` argument:

```{r}
predictions(mod, newdata = "mean", hypothesis = .5)
```

This can obviously be useful in other contexts. For instance, if we compute risk ratios (at the mean) associated with an increase of 1 unit in `hp`, it makes more sense to test the null hypothesis that the ratio of predictions is 1 rather than 0:

```{r}
comparisons(
    mod,
    newdata = "mean",
    variables = "hp",
    comparison = "ratio",
    hypothesis = 1) |>
    print(digits = 3)
```

Warning: Z statistics and p values are computed *before* applying functions in `transform`.

## Hypothesis tests with the delta method

The `marginaleffects` package includes a powerful function called `hypotheses()`. This function emulates the behavior of the well-established `car::deltaMethod` and `car::linearHypothesis` functions, but it supports more models, requires fewer dependencies, and offers some convenience features like shortcuts for robust standard errors.

`hypotheses()` can be used to compute estimates and standard errors of arbitrary functions of model parameters. For example, it can be used to conduct tests of equality between coefficients, or to test the value of some linear or non-linear combination of quantities of interest. `hypotheses()` can also be used to conduct hypothesis tests on other functions of a model's parameter, such as adjusted predictions or marginal effects.

Let's start by estimating a simple model:

```{r}
library(marginaleffects)
mod <- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)
```

When the `FUN` and `hypothesis` arguments of `hypotheses()` equal `NULL` (the default), the function returns a data.frame of raw estimates:

```{r}
hypotheses(mod)
```

Test of equality between coefficients:

```{r}
hypotheses(mod, "hp = wt")
```

Non-linear function of coefficients

```{r}
hypotheses(mod, "exp(hp + wt) = 0.1")
```

The `vcov` argument behaves in the same was as in the `slopes()` function. It allows us to easily compute robust standard errors:

```{r}
hypotheses(mod, "hp = wt", vcov = "HC3")
```

We can use shortcuts like `b1`, `b2`, `...` to identify the position of each parameter in the output of `FUN`. For example, `b2=b3` is equivalent to `hp=wt` because those term names appear in the 2nd and 3rd row when we call `hypotheses(mod)`. 

```{r}
hypotheses(mod, "b2 = b3")
```

```{r}
hypotheses(mod, hypothesis = "b* / b3 = 1")
```

Term names with special characters must be enclosed in backticks:

```{r}
hypotheses(mod, "`factor(cyl)6` = `factor(cyl)8`")
```

### Arbitrary functions: `FUN`

The `FUN` argument can be used to compute standard errors for arbitrary functions of model parameters. This user-supplied function must accept a single model object, and return a numeric vector or a data.frame with two columns named `term` and `estimate`.

```{r}
mod <- glm(am ~ hp + mpg, data = mtcars, family = binomial)

f <- function(x) {
    out <- x$coefficients["hp"] + x$coefficients["mpg"]
    return(out)
}
hypotheses(mod, FUN = f)
```

With labels:

```{r}
f <- function(x) {
    out <- data.frame(
        term = "Horsepower + Miles per Gallon",
        estimate = x$coefficients["hp"] + x$coefficients["mpg"]
    )
    return(out)
}
hypotheses(mod, FUN = f)
```

Test of equality between two predictions (row 2 vs row 3):

```{r}
f <- function(x) predict(x, newdata = mtcars)
hypotheses(mod, FUN = f, hypothesis = "b2 = b3")
```

Note that we specified the `newdata` argument in the `f` function. This is because the `predict()` method associated with `lm` objects will automatically the original fitted values when `newdata` is `NULL`, instead of returning the slightly altered fitted values which we need to compute numerical derivatives in the delta method.

We can also use numeric vectors to specify linear combinations of parameters. For example, there are 3 coefficients in the last model we estimated. To test the null hypothesis that the sum of the 2nd and 3rd coefficients is equal to 0, we can do:

```{r}
hypotheses(mod, hypothesis = c(0, 1, 1))
```

See below for more example of how to use string formulas, numeric vectors, or matrices to calculate custom contrasts, linear combinations, and linear or non-linear hypothesis tests.

### Arbitrary quantities with data frames

`marginaleffects` can also compute uncertainty estimates for arbitrary quantities hosted in a data frame, as long as the user can supply a variance-covariance matrix. (Thanks to Kyle F Butts for this cool feature and example!)

Say you run a monte-carlo simulation and you want to perform hypothesis of various quantities returned from each simulation. The quantities are correlated within each draw:

```{r} 
# simulated means and medians
draw <- function(i) { 
  x <- rnorm(n = 10000, mean = 0, sd = 1)
  out <- data.frame(median = median(x), mean =  mean(x))
  return(out)
}
sims <- do.call("rbind", lapply(1:25, draw))

# average mean and average median 
coeftable <- data.frame(
  term = c("median", "mean"),
  estimate = c(mean(sims$median), mean(sims$mean))
)

# variance-covariance
vcov <- cov(sims)

# is the median equal to the mean?
hypotheses(
  coeftable,
  vcov = vcov,
  hypothesis = "median = mean"
)
```


## `hypotheses` Formulas

Each of the 4 core functions of the package support a `hypothesis` argument which behaves similarly to the `hypotheses()` function. This argument allows users to specify custom hypothesis tests and contrasts, in order to test null hypotheses such as:

* The coefficients $\beta_1$ and $\beta_2$ are equal.
* The marginal effects of $X_1$ and $X_2$ equal.
* The marginal effect of $X$ when $W=0$ is equal to the marginal effect of $X$ when $W=1$.
* A non-linear function of adjusted predictions is equal to 100.
* The marginal mean in the control group is equal to the average of marginal means in the other 3 treatment arms.
* Cross-level contrasts: In a multinomial model, the effect of $X$ on the 1st outcome level is equal to the effect of $X$ on the 2nd outcome level.

### Marginal effects

For example, let's fit a model and compute some [marginal effects at the mean:](slopes.html#marginal-effect-at-the-mean-mem)

```{r}
library(marginaleffects)

mod <- lm(mpg ~ am + vs, data = mtcars)

mfx <- slopes(mod, newdata = "mean")
mfx
```

Is the marginal effect of `am` different from the marginal effect of `vs`? To answer this question we can run a linear hypothesis test using the `hypotheses` function:

```{r}
hypotheses(mfx, hypothesis = "am = vs")
```

Alternatively, we can specify the hypothesis directly in the original call:

```{r}
library(marginaleffects)

mod <- lm(mpg ~ am + vs, data = mtcars)

slopes(
    mod,
    newdata = "mean",
    hypothesis = "am = vs")
```

The `hypotheses` string can include any valid `R` expression, so we can run some silly non-linear tests:

```{r}
slopes(
    mod,
    newdata = "mean",
    hypothesis = "exp(am) - 2 * vs = -400")
```

But note that the p values and confidence intervals are calculated using the delta method and are thus based on the assumption that the `hypotheses` expression is approximately normally distributed. For (very) non-linear functions of the parameters, this is not realistic, and we get p values with incorrect error rates and confidence intervals with incorrect coverage probabilities. For such hypotheses, it’s better to calculate the confidence intervals using the bootstrap (see [`inferences`](reference/inferences.html) for details):

```{r}
set.seed(1234)
slopes(
    mod,
    newdata = "mean",
    hypothesis = "exp(am) - 2 * vs = -400") |>
  inferences(method = "boot")
```

While the confidence interval from the delta method is symmetric (equal to the estimate ± 1.96 times the standard error), the more reliable confidence interval from the bootstrap is (here) highly skewed.


### Adjusted Predictions

Now consider the case of adjusted predictions:

```{r}
p <- predictions(
    mod,
    newdata = datagrid(am = 0:1, vs = 0:1))
p
```

Since there is no `term` column in the output of the `predictions` function, we must use parameter identifiers like `b1`, `b2`, etc. to determine which estimates we want to compare:

```{r}
hypotheses(p, hypothesis = "b1 = b2")
```

Or directly:

```{r}
predictions(
    mod,
    hypothesis = "b1 = b2",
    newdata = datagrid(am = 0:1, vs = 0:1))

p$estimate[1] - p$estimate[2]
```

In the next section, we will see that we can get equivalent results by using a vector of contrast weights, which will be used to compute a linear combination of estimates:

```{r}
predictions(
    mod,
    hypothesis = c(1, -1, 0, 0),
    newdata = datagrid(am = 0:1, vs = 0:1))
```

There are *many* more possibilities:

```{r}
predictions(
    mod,
    hypothesis = "b1 + b2 = 30",
    newdata = datagrid(am = 0:1, vs = 0:1))

p$estimate[1] + p$estimate[2] - 30

predictions(
    mod,
    hypothesis = "(b2 - b1) / (b3 - b2) = 0",
    newdata = datagrid(am = 0:1, vs = 0:1))
```

### Average contrasts or marginal effects

The standard workflow with the `marginaleffects` package is to call a function like `predictions()`, `slopes()` or `comparisons()` to compute unit-level quantities; or one of their cousins `avg_predictions()`, `avg_comparisons()`, or `avg_slopes()` to aggregate the unit-level quantities into "Average Marginal Effects" or "Average Contrasts." We can also use the `comparison` argument to emulate the behavior of the `avg_*()` functions.

First, note that these three commands produce the same results:

```{r}
comparisons(mod, variables = "vs")$estimate |> mean()

avg_comparisons(mod, variables = "vs")

comparisons(
    mod,
    variables = "vs",
    comparison = "differenceavg")
```

[See the transformations section of the Contrasts vignette for more details.](comparisons.html)

With these results in hand, we can now conduct a linear hypothesis test between average marginal effects:

```{r, warning = FALSE}
comparisons(
    mod,
    hypothesis = "am = vs",
    comparison = "differenceavg")
```

Computing contrasts between average marginal effects requires a little care to obtain the right scale. In particular, we need to specify both the `variables` and the `comparison`:

```{r, warning = FALSE}
comparisons(
    mod,
    hypothesis = "am = vs",
    variables = c("am", "vs"),
    comparison = "dydxavg")
```

### Generic Hypothesis for Unsupported S3 Objects

`marginaleffects` provides a generic interface for hypothesis tests for linear models by providing (1) a data.frame containing point estimates (consiting of columns `term` containing the names and `estimate` containing the point estiamtes) and (2) a variance-covariance matrix of estimates.

```{r}
coeftable <- data.frame(term = names(mod$coefficients), estimate = as.numeric(mod$coefficients))
vcov <- vcov(mod)

hypotheses(
  coeftable, vcov = vcov, 
  hypothesis = "am = vs"
)
```

## `hypotheses` Vectors and Matrices

The `marginal_means()` function computes [estimated marginal means.](marginalmeans.html) The `hypothesis` argument of that function offers a powerful mechanism to estimate custom contrasts between marginal means, by way of linear combination.

Consider a simple example:

```{r}
library(marginaleffects)
library(emmeans)
library(nnet)

dat <- mtcars
dat$carb <- factor(dat$carb)
dat$cyl <- factor(dat$cyl)
dat$am <- as.logical(dat$am)

mod <- lm(mpg ~ carb + cyl, dat)
mm <- marginal_means(mod, variables = "carb")
mm
```

The contrast between marginal means for `carb==1` and `carb==2` is:

```{r}
21.66232 - 21.34058 
```

or

```{r}
21.66232 + -(21.34058)
```

or

```{r}
sum(c(21.66232, 21.34058) * c(1, -1))
```

or 

```{r}
c(21.66232, 21.34058) %*% c(1, -1)
```

The last two commands express the contrast of interest as [a linear combination](https://en.wikipedia.org/wiki/Linear_combination) of marginal means.

### Simple contrast

In the `marginal_means()` function, we can supply a `hypothesis` argument to compute linear combinations of marginal means. This argument must be a numeric vector of the same length as the number of rows in the output of `marginal_means()`. For example, in the previous there were six rows, and the two marginal means we want to compare are at in the first two positions:

```{r}
lc <- c(1, -1, 0, 0, 0, 0)
marginal_means(mod, variables = "carb", hypothesis = lc)
```

### Complex contrast

Of course, we can also estimate more complex contrasts:

```{r}
lc <- c(0, -2, 1, 1, -1, 1)
marginal_means(mod, variables = "carb", hypothesis = lc)
```

`emmeans` produces similar results:

```{r}
library(emmeans)
em <- emmeans(mod, "carb")
lc <- data.frame(custom_contrast = c(-2, 1, 1, 0, -1, 1))
contrast(em, method = lc)
```

### Multiple contrasts

Users can also compute multiple linear combinations simultaneously by supplying a numeric matrix to `hypotheses`. This matrix must have the same number of rows as the output of `slopes()`, and each column represents a distinct set of weights for different linear combinations. The column names of the matrix become labels in the output. For example:

```{r}
lc <- matrix(c(
    -2, 1, 1, 0, -1, 1,
    1, -1, 0, 0, 0, 0
    ), ncol = 2)
colnames(lc) <- c("Contrast A", "Contrast B")
lc

marginal_means(mod, variables = "carb", hypothesis = lc)
```

### Contrasts across response levels

In models with multinomial outcomes, one may be interested in comparing outcomes or contrasts across response levels. For example, in this model there are 18 estimated marginal means, across 6 outcome levels (the `group` column):

```{r}
library(nnet)
mod <- multinom(carb ~ mpg + cyl, data = dat, trace = FALSE)
mm <- marginal_means(mod, type = "probs")
mm
```

Let's contrast the marginal means in the first outcome level when `cyl` equals 4 and 6. These marginal means are located in rows 1 and 7 respectively:

```{r}
lc <- rep(0, nrow(mm))
lc[1] <- -1
lc[7] <- 1
marginal_means(
    mod,
    type = "probs",
    hypothesis = lc)
```

This is indeed equal to the results we would have obtained manually:

```{r}
2.828726e-01 - 3.678521e-01
```

Now let's say we want to calculate a "contrast in contrasts", that is, the outcome of a 3-step process:

1. Contrast between `cyl=6` and `cyl=4` in the 1st outcome level
2. Contrast between `cyl=6` and `cyl=4` in the 2nd outcome level
3. Contrast between the contrasts defined in steps 1 and 2.

We create the linear combination weights as follows:

```{r}
lc <- rep(0, nrow(mm))
lc[c(1, 8)] <- -1
lc[c(7, 2)] <- 1
```

To make sure that the weights are correct, we can display them side by side with the original `marginal_means()` output:

```{r}
transform(mm[, 1:3], lc = lc)
```

Compute the results:

```{r}
marginal_means(mod, type = "probs", hypothesis = lc)
```

## Pairwise contrasts: Difference-in-Differences

Now we illustrate how to use the machinery described above to do pairwise comparisons between contrasts, a type of analysis often associated with a "Difference-in-Differences" research design.

First, we simulate data with two treatment groups and pre/post periods:

```{r, message = FALSE}
library(data.table)

N <- 1000
did <- data.table(
    id = 1:N,
    pre = rnorm(N),
    trt = sample(0:1, N, replace = TRUE))
did$post <- did$pre + did$trt * 0.3 + rnorm(N)
did <- melt(
    did,
    value.name = "y",
    variable.name = "time",
    id.vars = c("id", "trt"))
head(did)
```

Then, we estimate a linear model with a multiple interaction between the time and the treatment indicators. We also compute contrasts at the mean for each treatment level:

```{r}
did_model <- lm(y ~ time * trt, data = did)

comparisons(
    did_model,
    newdata = datagrid(trt = 0:1),
    variables = "time")
```

Finally, we compute pairwise differences between contrasts. This is the Diff-in-Diff estimate:

```{r}
comparisons(
    did_model,
    variables = "time",
    newdata = datagrid(trt = 0:1),
    hypothesis = "pairwise")
```

## Joint hypotheses tests

The `hypotheses()` function can also test multiple hypotheses jointly. For example, consider this model:

```{r}
model <- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)
coef(model)
```

We may want to test the null hypothesis that two of the coefficients are jointly (both) equal to zero.

```{r}

hypotheses(model, joint = c("as.factor(cyl)6:hp", "as.factor(cyl)8:hp"))

```

The `joint` argument allows users to flexibly specify the parameters to be tested, using character vectors, integer indices, or Perl-compatible regular expressions. We can also specify the null hypothesis for each parameter individually using the `hypothesis` argument.

Naturally, the `hypotheses` function also works with `marginaleffects` objects.

```{r}
# ## joint hypotheses: regular expression
hypotheses(model, joint = "cyl")

## joint hypotheses: integer indices
hypotheses(model, joint = 2:3)

## joint hypotheses: different null hypotheses
hypotheses(model, joint = 2:3, hypothesis = 1)
hypotheses(model, joint = 2:3, hypothesis = 1:2)

## joint hypotheses: marginaleffects object
cmp <- avg_comparisons(model)
hypotheses(cmp, joint = "cyl")
```

We can also combine multiple calls to `hypotheses` to execute a joint test on linear combinations of coefficients:

```{r}
## fit model
mod <- lm(mpg ~ factor(carb), mtcars)

## hypothesis matrix for linear combinations
H <- matrix(0, nrow = length(coef(mod)), ncol = 2)
H[2:3, 1] <- H[4:6, 2] <- 1

## test individual linear combinations
hyp <- hypotheses(mod, hypothesis = H)
hyp

## test joint hypotheses
#hypotheses(hyp, joint = TRUE, hypothesis = c(-10, -20))
```

