---
title: "Marginal Means" 
---

```{r, include = FALSE}
options(width = 1000)
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .4,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
library("tidyverse")
library("kableExtra")
```

In the context of this package, "marginal means" refer to the values obtained by this three step process:

1. Construct a "grid" of predictor values with all combinations of categorical variables, and where numeric variables are held at their means.
2. Calculate adjusted predictions for each cell in that grid.
3. Take the average of those adjusted predictions across one dimension of the grid to obtain the marginal means.

For example, consider a model with a numeric, a factor, and a logical predictor:

```{r}
library(marginaleffects)

dat <- mtcars
dat$cyl <- as.factor(dat$cyl)
dat$am <- as.logical(dat$am)
mod <- lm(mpg ~ hp + cyl + am, data = dat)
```

Using the `predictions` function, we set the `hp` variable at its mean and compute predictions for all combinations for `am` and `cyl`:

```{r}
p <- predictions(
    mod,
    newdata = datagrid(am = unique, cyl = unique))
```

For illustration purposes, it is useful to reshape the above results:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(kableExtra)
pred <- p |>
    select(cyl, am, estimate) |>
    pivot_wider(names_from = "am", values_from = "estimate") |>
    rowwise() |>
    mutate(`Marginal means by cyl` = mean(c(`TRUE`, `FALSE`)))
row <- data.frame(x = "Marginal means by am",
                  y = mean(pred[["TRUE"]]),
                  z = mean(pred[["FALSE"]]))
colnames(row) <- colnames(pred)[1:3]
pred <- bind_rows(pred, row)
for (i in 2:ncol(pred)) {
    pred[[i]] <- sprintf("%.1f", pred[[i]])
}
pred[pred == "NA"] <- ""
kbl(pred) |> 
    kable_styling() |>
    add_header_above(c(" " = 1, "am" = 2, " " = 1))
```

The marginal means by `am` and `cyl` are obtained by taking the mean of the adjusted predictions across cells. The `marginal_means` function gives us the same results easily:

```{r}
marginal_means(mod)
```

We could obtain the same results with the `by` argument of the `predictions()` function, which allows us to marginalize across some predictors:

```{r}
predictions(
    mod,
    by = "am",
    newdata = datagrid(am = unique, cyl = unique))

predictions(
    mod,
    by = "cyl",
    newdata = datagrid(am = unique, cyl = unique))
```

The same results can be achieved using the [powerful `emmeans` package](https://cran.r-project.org/package=emmeans):

```{r}
library(emmeans)
emmeans(mod, specs = "cyl")
emmeans(mod, specs = "am")
```

## Marginal Means vs. Average Predictions

What should scientists report? Marginal means or average predictions? 

Many analysts ask this question, but unfortunately there isn't a single answer. As explained above, marginal means are a special case of predictions, made on a perfectly balanced grid of categorical predictors, with numeric predictors held at their means, and marginalized with respect to some focal variables. Whether the analyst prefers to report this specific type of marginal means or another kind of average prediction will depend on the characteristics of the sample and the population to which they want to generalize.

After reading this vignette and the discussion of `emmeans` in the [Alternative Software vignette](alternative_software.html), you may want to consult with a statistician to discuss your specific real-world problem and make an informed choice.

## Interactions

By default, the `marginal_means()` function calculates marginal means for each categorical predictor one after the other. We can also compute marginal means for *combinations* of categories by setting `cross=TRUE`:

```{r, message = FALSE, warning = FALSE}
library(lme4)

dat <- "https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv"
dat <- read.csv(dat)
titanic <- glmer(
    Survived ~ Sex * PClass + Age + (1 | PClass),
    family = binomial,
    data = dat)
```

Regardless of the scale of the predictions (`type` argument), `marginal_means()` always computes standard errors using the Delta Method: 

```{r}
marginal_means(
    titanic,
    type = "response",
    variables = c("Sex", "PClass"))
```

When the model is linear or on the link scale, it also produces confidence intervals: 

```{r}
marginal_means(
    titanic,
    type = "link",
    variables = c("Sex", "PClass"))
```

It is easy to transform those link-scale marginal means with arbitrary functions using the `transform` argument:

```{r}
marginal_means(
    titanic,
    type = "link",
    transform = insight::link_inverse(titanic),
    variables = c("Sex", "PClass"))
```

`marginal_means()` defaults to reporting EMMs for each category individually, without cross-margins:

```{r, warning = FALSE, message = FALSE}
titanic2 <- glmer(
    Survived ~ Sex + PClass + Age + (1 | PClass),
    family = binomial,
    data = dat)

marginal_means(
    titanic2,
    variables = c("Sex", "PClass"))
```

We can force the cross:

```{r}
marginal_means(
    titanic2,
    cross = TRUE,
    variables = c("Sex", "PClass"))
```

## Group averages with the `by` argument

We can collapse marginal means via averaging using the `by` argument:

```{r}
dat <- mtcars
dat$am <- factor(dat$am)
dat$vs <- factor(dat$vs)
dat$cyl <- factor(dat$cyl)

mod <- glm(gear ~ cyl + vs + am, data = dat, family = poisson)

by <- data.frame(
    by = c("(4 & 6)", "(4 & 6)", "(8)"),
    cyl = c(4, 6, 8))

marginal_means(mod, by = by, variables = "cyl")
```

And we can use the `hypothesis` argument to compare those new collapsed subgroups:

```{r}
marginal_means(mod, by = by, variables = "cyl", hypothesis = "pairwise")
```

## Custom Contrasts and Linear Combinations

See the vignette on [Custom Contrasts and Combinations](hypothesis.html)

## Tidy summaries

The `summary`, `tidy`, and `glance` functions are also available to summarize and manipulate the results:

```{r}
mm <- marginal_means(mod)

tidy(mm)

glance(mm)

summary(mm)
```

Thanks to those tidiers, we can also present the results in the style of a regression table [using the `modelsummary` package.](tables.html)


## Case study: Multinomial Logit

To begin, we generate data and estimate a large model:

```{r}
library(nnet)
library(marginaleffects)

set.seed(1839)
n <- 1200
x <- factor(sample(letters[1:3], n, TRUE))
y <- vector(length = n)
y[x == "a"] <- sample(letters[4:6], sum(x == "a"), TRUE)
y[x == "b"] <- sample(letters[4:6], sum(x == "b"), TRUE, c(1 / 4, 2 / 4, 1 / 4))
y[x == "c"] <- sample(letters[4:6], sum(x == "c"), TRUE, c(1 / 5, 3 / 5, 2 / 5))

dat <- data.frame(x = x, y = factor(y))
tmp <- as.data.frame(replicate(20, factor(sample(letters[7:9], n, TRUE))))
dat <- cbind(dat, tmp)
void <- capture.output({
    mod <- multinom(y ~ ., dat)
})
```

Try to compute marginal means, but realize that your grid wonâ€™t fit in memory:

```{r, error = TRUE}
marginal_means(mod, type = "probs")
```

Use the `variables` and `variables_grid` arguments to compute marginal means over a more reasonably sized grid:

```{r, eval = FALSE}
marginal_means(mod,
              type = "probs",
              variables = c("x", "V1"),
              variables_grid = paste0("V", 2:3))
```


## Plot conditional marginal means

The `marginaleffects` package offers several functions to plot how some quantities vary as a function of others:

* `plot_predictions`: Conditional adjusted predictions -- how does the predicted outcome change as a function of regressors?
* `plot_comparisons`: Conditional comparisons -- how do contrasts change as a function of regressors?
* `plot_slopes`: Conditional marginal effects -- how does the slope change as a function of regressors?

There is no analogous function for marginal means. However, it is very easy to achieve a similar effect using the `predictions()` function, its `by` argument, and standard plotting functions. In the example below, we take these steps:

1. Estimate a model with one continuous (`hp`) and one categorical regressor (`cyl`).
1. Create a perfectly "balanced" data grid for each combination of `hp` and `cyl`. This is specified by the user in the `datagrid()` call.
1. Compute fitted values (aka "adjusted predictions") for each cell of the grid.
1. Use the `by` argument to take the average of predicted values for each value of `hp`, across margins of `cyl`.
1. Compute standard errors around the averaged predicted values (i.e., marginal means).
1. Create symmetric confidence intervals in the usual manner.
1. Plot the results.

```{r}
library(ggplot2)

mod <- lm(mpg ~ hp + factor(cyl), data = mtcars)

p <- predictions(mod,
    by = "hp",
    newdata = datagrid(
        model = mod,
        hp = seq(100, 120, length.out = 10),
        cyl = mtcars$cyl))

ggplot(p) +
    geom_ribbon(aes(hp, ymin = conf.low, ymax = conf.high), alpha = .2) +
    geom_line(aes(hp, estimate))
```