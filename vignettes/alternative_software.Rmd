---
title: "Comparisons to alternative software" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Comparisons to alternative software}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .4,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

If you do not like `marginaleffects`, you may want to consider one of the alternatives described below:

* `margins`: https://cran.r-project.org/web/packages/margins/index.html
* `prediction`: https://cran.r-project.org/web/packages/prediction/index.html
* `emmeans`: https://cran.r-project.org/web/packages/emmeans/index.html
* `brmsmargins`: https://joshuawiley.com/brmsmargins/
* `effects`: https://cran.r-project.org/web/packages/effects/index.html
* `modelbased`: https://easystats.github.io/modelbased/
* `ggeffects`: https://strengejacke.github.io/ggeffects/
* `Stata` by StataCorp LLC

# `emmeans`

The [`emmeans` package](https://cran.r-project.org/web/packages/emmeans/index.html) is developed by Russell V. Lenth and colleagues. It is an extremely powerful package which focuses on the computation of marginal means, but which can also compute slopes. 

In my (Vincent's) biased opinion, the main benefits of `marginaleffects` over `emmeans`:

* Support more model types.
* Simpler and more intuitive user interface.
* Easier to compute average marginal effects and unit-level marginal effects for whole datasets.
* Easier to compute marginal effects (slopes) for custom grids and continuous regressors.
* Common plots are easy with the `plot_cap()` and `plot_cme()` functions.

Many of `marginaleffects` advantages come down to subjective preferences over user interface. Readers are thus encouraged to try both packages to see which interface they prefer. Please keep in mind that `emmeans` offers several features which are not yet available in `marginaleffects`, including:

* Equivalence tests.
* Multiplicity adjustments.
* Some plots.

The [Marginal Means Vignette](https://vincentarelbundock.github.io/marginaleffects/articles/marginalmeans.html) includes side-by-side comparisons of `emmeans` and `marginaleffects` to compute marginal means. The rest of this section compares the syntax for contrasts and marginaleffects.

## Contrasts

As far as I can tell, `emmeans` does not provide an easy way to compute unit-level contrasts for every row of the dataset used to fit our model. Therefore, the side-by-side syntax shown below will always include `newdata=datagrid()` to specify that we want to compute only one contrast: at the mean values of the regressors. In day-to-day practice with `marginaleffects()`, however, this extra argument would not be necessary.

Fit a model:
```{r}
library(emmeans)
library(marginaleffects)

mod <- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)
```

Response scale, reference groups:

```{r}
emm <- emmeans(mod, specs = "cyl", regrid = "response")
contrast(emm, method = "trt.vs.ctrl1")

comparisons(mod, newdata = "mean")
```

Link scale, pairwise contrasts:

```{r}
emm <- emmeans(mod, specs = "cyl")
contrast(emm, method = "revpairwise")

comparisons(mod,
            type = "link",
            newdata = "mean",
            variables = list(cyl = "pairwise"))
```

## Contrasts by group

Here is a slightly more complicated example with contrasts estimated by subgroup in a `lme4` mixed effects model. First we estimate a model and compute pairwise contrasts by subgroup using `emmeans`:

```{r}
library(dplyr)
library(lme4)
library(emmeans)

dat <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/lme4/VerbAgg.csv")
dat$woman <- as.numeric(dat$Gender == "F")

mod <- glmer(
    woman ~ btype * resp + situ + (1 + Anger | item),
    family = binomial,
    data = dat)

emmeans(mod, specs = "btype", by = "resp") |>
    contrast(method = "revpairwise", adjust = "none")
```

What did `emmeans` do to obtain these results? Roughly speaking:

1. Create a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.
2. Make adjusted predictions in each cell of the prediction grid.
3. Take the average of those predictions (marginal means) for each combination of `btype` (focal variable) and `resp` (group `by` variable).
4. Compute pairwise differences (contrasts) in marginal means across different levels of the focal variable `btype`.

In short, `emmeans` computes pairwise contrasts between *marginal means*, which are themselves averages of adjusted predictions. This is different from the default types of contrasts produced by `comparisons()`, which reports contrasts between adjusted predictions, *without* averaging across a pre-specified grid of predictors. What does `comparisons()` do instead? 

Let `newdata` be a data frame supplied by the user (or the original data frame used to fit the model), then:

1. Create a new data frame called `newdata2`, which is identical to `newdata` except that the focal variable is incremented by one level.
2. Compute contrasts as the difference between adjusted predictions made on the two datasets: 
   - `predict(model, newdata = newdata2) - predict(model, newdata = newdata)`

Although it is not idiomatic, we can use still use `comparisons()` to emulate the `emmeans` results. First, we create a prediction grid with one cell for each combination of categorical predictor in the model:

```{r}
nd <- datagrid(
    model = mod,
    resp = dat$resp,
    situ = dat$situ,
    btype = dat$btype)
nrow(nd)
```

This grid has 18 rows, one for each combination of levels for the `resp` (3), `situ` (2), and `btype` (3) variables (3 * 2 * 3 = 18). 

Then we compute pairwise contrasts over this grid:

```{r}
cmp <- comparisons(mod,
    variables = list("btype" = "pairwise"),
    newdata = nd,
    type = "link")
nrow(cmp)
```

There are 3 pairwise contrasts, corresponding to the 3 pairwise comparisons possible between the 3 levels of the focal variable `btype`: `scold-curse`, `shout-scold`, `shout-curse`. The `comparisons()` function estimates those 3 contrasts for each row of `newdata`, so we get $18 \times 3 = 54$ rows.

Finally, if we wanted contrasts averaged over each subgroup of the `resp` variable, we can use the `by` argument:

```{r}
comparisons(mod,
    by = "resp",
    variables = list("btype" = "pairwise"),
    newdata = nd,
    type = "link") |>
    summary()
```

These results are identical to those produced by `emmeans` (except for $t$ vs. $z$).

## Marginal Effects

As far as I can tell, `emmeans::emtrends` makes it easier to compute marginal effects for a few user-specified values than for large grids or for the full original dataset.

Response scale, user-specified values:

```{r}
mod <- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)

emtrends(mod, ~hp, "hp", regrid = "response", at = list(cyl = 4))

marginaleffects(mod, newdata = datagrid(cyl = 4))
```

Link scale, user-specified values:

```{r}
emtrends(mod, ~hp, "hp", at = list(cyl = 4))

marginaleffects(mod, type = "link", newdata = datagrid(cyl = 4))
```

# `margins` and `prediction`

The [`margins`](https://cran.r-project.org/web/packages/margins/index.html) and [`prediction`](https://cran.r-project.org/web/packages/prediction/index.html) packages for `R` were designed by Thomas Leeper to emulate the behavior of the `margins` command from `Stata`. These packages are trailblazers and strongly influenced the development of `marginaleffects`. The main benefits of `marginaleffects` over these packages are:

* Support more model types
* Faster
* Memory efficient
* Plots using `ggplot2` instead of Base R
* More extensive test suite
* Active development

The syntax of the two packages is very similar.

## Average Marginal Effects

```{r}
library(margins)
library(marginaleffects)

mod <- lm(mpg ~ cyl + hp + wt, data = mtcars)

mar <- margins(mod)
summary(mar)

mfx <- marginaleffects(mod)
summary(mfx)
```

## Individual-Level Marginal Effects

Marginal effects in a user-specified data frame:

```{r}
head(data.frame(mar))

head(mfx)
nd <- data.frame(cyl = 4, hp = 110, wt = 3)
```

## Marginal Effects at the Mean

```{r}
mar <- margins(mod, data = data.frame(mean_or_mode(mtcars)), unit_ses = TRUE)
data.frame(mar)

marginaleffects(mod, newdata = "mean")
```

## Counterfactual Average Marginal Effects

The `at` argument of the `margins` package emulates `Stata` by fixing the values of some variables at user-specified values, and by replicating the full dataset several times for each combination of the supplied values (see the `Stata` section below). For example, if the dataset includes 32 rows and the user calls `at=list(cyl=c(4, 6))`, `margins` will compute 64 unit-level marginal effects estimates:

```{r}
dat <- mtcars
dat$cyl <- factor(dat$cyl)
mod <- lm(mpg ~ cyl * hp + wt, data = mtcars)

mar <- margins(mod, at = list(cyl = c(4, 6, 8)))
summary(mar)

mfx <- marginaleffects(
    mod,
    by = "cyl",
    newdata = datagridcf(cyl = c(4, 6, 8)))
summary(mfx)
```

## Adjusted Predictions

The syntax to compute adjusted predictions using the `predictions` package or `marginaleffects` is very similar:

```{r}
prediction::prediction(mod) |> head()

marginaleffects::predictions(mod) |> head()
```

# `Stata`

`Stata` is a good but expensive software package for statistical analysis. It is published by StataCorp LLC. This section compares `Stata`'s `margins` command to `marginaleffects`.

The results produced by `marginaleffects` are extensively tested against `Stata`. See the [test suite](https://github.com/vincentarelbundock/marginaleffects/tree/main/tests/testthat/stata) for a list of the dozens of models where we compared estimates and standard errors.

## Average Marginal Effect (AMEs)

Marginal effects are unit-level quantities. To compute "average marginal effects", we first calculate marginal effects for each observation in a dataset. Then, we take the mean of those unit-level marginal effects. 

### Stata

Both Stata's `margins` command and the `marginaleffects` function can calculate average marginal effects (AMEs). Here is an example showing how to estimate AMEs in Stata: 

```
quietly reg mpg cyl hp wt
margins, dydx(*)

Average marginal effects                        Number of obs     =         32
Model VCE    : OLS
 
Expression   : Linear prediction, predict()
dy/dx w.r.t. : cyl hp wt
 
------------------------------------------------------------------------------
    |            Delta-method
    |      dy/dx   Std. Err.      t    P>|t|     [95% Conf. Interval]
------------------------------------------------------------------------------
cyl |  -.9416168   .5509164    -1.71   0.098    -2.070118    .1868842
 hp |  -.0180381   .0118762    -1.52   0.140    -.0423655    .0062893
 wt |  -3.166973   .7405759    -4.28   0.000    -4.683974   -1.649972
------------------------------------------------------------------------------
```

### marginaleffects

The same results can be obtained with `marginaleffects()` and `summary()` like this:

```{r}
library("marginaleffects")
mod <- lm(mpg ~ cyl + hp + wt, data = mtcars)
mfx <- marginaleffects(mod)
summary(mfx)
```

Note that `Stata` reports t statistics while `marginaleffects` reports Z. This produces slightly different p-values because this model has low degrees of freedom: `mtcars` only has 32 rows

## Counterfactual Marginal Effects

A "counterfactual marginal effect" is a special quantity obtained by replicating a dataset while fixing some regressor to user-defined values.

Concretely, Stata computes counterfactual marginal effects in 3 steps:

  1. Duplicate the whole dataset 3 times and sets the values of `cyl` to the three specified values in each of those subsets.
  2. Calculate marginal effects for each observation in that large grid.
  3. Take the average of marginal effects for each value of the variable of interest. 

### Stata

With the `at` argument, Stata's `margins` command estimates average _counterfactual_ marginal effects. Here is an example:

```
quietly reg mpg i.cyl##c.hp wt
margins, dydx(hp) at(cyl = (4 6 8))

Average marginal effects                        Number of obs     =         32
Model VCE    : OLS

Expression   : Linear prediction, predict()
dy/dx w.r.t. : hp

1._at        : cyl             =           4

2._at        : cyl             =           6

3._at        : cyl             =           8

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
hp           |
         _at |
          1  |   -.099466   .0348665    -2.85   0.009    -.1712749   -.0276571
          2  |  -.0213768    .038822    -0.55   0.587    -.1013323    .0585787
          3  |   -.013441   .0125138    -1.07   0.293    -.0392137    .0123317
------------------------------------------------------------------------------
```

<!-- Adapted from this GitHub issue: https://github.com/strengejacke/ggeffects/issues/249 -->

### marginaleffects

You can estimate average counterfactual marginal effects with `marginaleffects()` by using the `datagridcf()` to create a counterfactual dataset in which the full original dataset is replicated for each potential value of the `cyl` variable. Then, we tell the `by` argument to average within groups:

```{r}
mod <- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)

mfx <- marginaleffects(
    mod,
    variables = "hp",
    by = "cyl",
    newdata = datagridcf(cyl = c(4, 6, 8)))
summary(mfx)
```

This is equivalent to taking the group-wise mean of observation-level marginal effects:

```{r}
aggregate(dydx ~ term + cyl, data = mfx, FUN = mean)
```

<!-- Taken from https://github.com/vincentarelbundock/marginaleffects/issues/226 -->

Note that following `Stata`, the standard errors for group-averaged marginal effects are computed by taking the "Jacobian at the mean:" 


```{r}
J <- attr(mfx, "jacobian")
J_mean <- aggregate(J, by = list(mfx$cyl), FUN = mean)
J_mean <- as.matrix(J_mean[, 2:ncol(J_mean)])
sqrt(diag(J_mean %*% vcov(mod) %*% t(J_mean)))
```

## Average Counterfactual Adjusted Predictions

### Stata

Just like Stata's `margins` command computes average counterfactual marginal effects, it can also estimate _average counterfactual adjusted predictions_.  

Here is an example:

```
quietly reg mpg i.cyl##c.hp wt
margins, at(cyl = (4 6 8))

Predictive margins                              Number of obs     =         32
Model VCE    : OLS

Expression   : Linear prediction, predict()

1._at        : cyl             =           4

2._at        : cyl             =           6

3._at        : cyl             =           8

------------------------------------------------------------------------------
             |            Delta-method
             |     Margin   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         _at |
          1  |   17.44233   2.372914     7.35   0.000     12.55522    22.32944
          2  |    18.9149   1.291483    14.65   0.000     16.25505    21.57476
          3  |   18.33318   1.123874    16.31   0.000     16.01852    20.64785
------------------------------------------------------------------------------
```

Again, this is what Stata does in the background:

  1. It duplicates the whole dataset 3 times and sets the values of `cyl` to the three specified values in each of those subsets.
  2. It calculates predictions for that large grid.
  3. It takes the average prediction for each value of `cyl`.
  
In other words, average counterfactual adjusted predictions as implemented by Stata are a hybrid between predictions at the observed values (the default in `marginaleffects::predictions`) and predictions at representative values. 

### marginaleffects

You can estimate average counterfactual adjusted predictions with `predictions()` by, first, setting the `grid_type` argument of `datagrid()` to `"counterfactual"` and, second, by averaging the predictions using the `by` argument of `summary()`, or a manual function like `dplyr::summarise()`.

```{r}
mod <- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)

pred <- predictions(
    mod,
    by = "cyl",
    newdata = datagridcf(cyl = c(4, 6, 8)))
summary(pred)

pred %>%
    group_by(cyl) %>%
    summarize(AAP = mean(predicted))
```

# `brmsmargins`

[The `brmsmargins` package](https://joshuawiley.com/brmsmargins/) is developed by Joshua Wiley:

> This package has functions to calculate marginal effects from brms models ( http://paul-buerkner.github.io/brms/ ). A central motivator is to calculate average marginal effects (AMEs) for continuous and discrete predictors in fixed effects only and mixed effects regression models including location scale models.

The main advantage of `brmsmargins` over `marginaleffects` is its ability to compute "Marginal Coefficients" following the method described in [Hedeker et al (2012).](https://doi.org/10.1111/biom.12707)

The main advantages of `marginaleffects` over `brmsmargins` are:

1. Support for 60+ model types, rather than just the `brms` package.
2. Simpler user interface (subjective).
3. At the time of writing (2022-05-25) `brmsmargins` did not support certain `brms` models such as those with multivariate or multinomial outcomes. It also did not support custom outcome transformations.

The rest of this section presents side-by-side replications of some of the analyses from the `brmsmargins` vignettes in order to show highlight parallels and differences in syntax.

## Marginal Effects for Fixed Effects Models

### AMEs for Logistic Regression

Estimate a logistic regression model with `brms`:

```{r, message = FALSE, warning = FALSE}
library(brms)
library(brmsmargins)
library(marginaleffects)
library(data.table)
library(withr)
h <- 1e-4

void <- capture.output(
    bayes.logistic <- brm(
      vs ~ am + mpg, data = mtcars,
      family = "bernoulli", seed = 1234,
      silent = 2, refresh = 0,
      chains = 4L, cores = 4L)
)
```

Compute AMEs manually:

```{r}
d1 <- d2 <- mtcars
d2$mpg <- d2$mpg + h
p1 <- posterior_epred(bayes.logistic, newdata = d1)
p2 <- posterior_epred(bayes.logistic, newdata = d2)
m <- (p2 - p1) / h
quantile(rowMeans(m), c(.5, .025, .975))
```


Compute AMEs with `brmsmargins`:

```{r}
bm <- brmsmargins(
  bayes.logistic,
  add = data.frame(mpg = c(0, 0 + h)),
  contrasts = cbind("AME MPG" = c(-1 / h, 1 / h)),
  CI = 0.95,
  CIType = "ETI")
data.frame(bm$ContrastSummary)
```

Compute AMEs using `marginaleffects`:

```{r}
mfx <- marginaleffects(bayes.logistic) 
summary(mfx)
```

The `mpg` element of the `Effect` column from `marginaleffects` matches the the `M` column of the output from `brmsmargins`.

## Marginal Effects for Mixed Effects Models

Estimate a mixed effects logistic regression model with `brms`:

```{r, message = FALSE, warning = TRUE}
d <- withr::with_seed(
  seed = 12345, code = {
    nGroups <- 100
    nObs <- 20
    theta.location <- matrix(rnorm(nGroups * 2), nrow = nGroups, ncol = 2)
    theta.location[, 1] <- theta.location[, 1] - mean(theta.location[, 1])
    theta.location[, 2] <- theta.location[, 2] - mean(theta.location[, 2])
    theta.location[, 1] <- theta.location[, 1] / sd(theta.location[, 1])
    theta.location[, 2] <- theta.location[, 2] / sd(theta.location[, 2])
    theta.location <- theta.location %*% chol(matrix(c(1.5, -.25, -.25, .5^2), 2))
    theta.location[, 1] <- theta.location[, 1] - 2.5
    theta.location[, 2] <- theta.location[, 2] + 1
    d <- data.table(
      x = rep(rep(0:1, each = nObs / 2), times = nGroups))
    d[, ID := rep(seq_len(nGroups), each = nObs)]

    for (i in seq_len(nGroups)) {
      d[ID == i, y := rbinom(
        n = nObs,
        size = 1,
        prob = plogis(theta.location[i, 1] + theta.location[i, 2] * x))
        ]
    }
    copy(d)
  })

void <- capture.output(
    mlogit <- brms::brm(
      y ~ 1 + x + (1 + x | ID), family = "bernoulli",
      data = d, seed = 1234,
      silent = 2, refresh = 0,
      chains = 4L, cores = 4L)
)
```

### AME: Including Random Effects

```{r}
bm <- brmsmargins(
  mlogit,
  add = data.frame(x = c(0, h)),
  contrasts = cbind("AME x" = c(-1 / h, 1 / h)),
  effects = "includeRE",
  CI = .95,
  CIType = "ETI")
data.frame(bm$ContrastSummary)

mfx <- marginaleffects(mlogit)
summary(mfx)
```

### AME: Fixed Effects Only (Grand Mean)

```{r}
bm <- brmsmargins(
  mlogit,
  add = data.frame(x = c(0, h)),
  contrasts = cbind("AME x" = c(-1 / h, 1 / h)),
  effects = "fixedonly",
  CI = .95,
  CIType = "ETI")
data.frame(bm$ContrastSummary)

mfx <- marginaleffects(mlogit, re_formula = NA)
summary(mfx)
```

## Marginal Effects for Location Scale Models

### AMEs for Fixed Effects Location Scale Models

Estimate a fixed effects location scale model with `brms`:

```{r, message = FALSE, warning = FALSE}
d <- withr::with_seed(
  seed = 12345, code = {
    nObs <- 1000L
    d <- data.table(
      grp = rep(0:1, each = nObs / 2L),
      x = rnorm(nObs, mean = 0, sd = 0.25))
    d[, y := rnorm(nObs,
                   mean = x + grp,
                   sd = exp(1 + x + grp))]
    copy(d)
  })

void <- capture.output(
    ls.fe <- brm(bf(
      y ~ 1 + x + grp,
      sigma ~ 1 + x + grp),
      family = "gaussian",
      data = d, seed = 1234,
      silent = 2, refresh = 0,
      chains = 4L, cores = 4L)
)
```

### Fixed effects only

```{r}
bm <- brmsmargins(
  ls.fe,
  add = data.frame(x = c(0, h)),
  contrasts = cbind("AME x" = c(-1 / h, 1 / h)),
  CI = 0.95, CIType = "ETI",
  effects = "fixedonly")
data.frame(bm$ContrastSummary)

mfx <- marginaleffects(ls.fe, re_formula = NA)
summary(mfx)
```

### Discrete change and distributional parameter (`dpar`)

Compute the contrast between adjusted predictions on the `sigma` parameter, when `grp=0` and `grp=1`:

```{r}
bm <- brmsmargins(
  ls.fe,
  at = data.frame(grp = c(0, 1)),
  contrasts = cbind("AME grp" = c(-1, 1)),
  CI = 0.95, CIType = "ETI", dpar = "sigma",
  effects = "fixedonly")
data.frame(bm$ContrastSummary)
```

In `marginaleffects` we use the `comparisons()` function and the `variables` argument:

```{r}
cmp <- comparisons(
  ls.fe,
  variables = list(grp = 0:1),
  dpar = "sigma")
summary(cmp)
```

### Marginal effect (continuous) on sigma

```{r}
bm <- brmsmargins(
  ls.fe,
  add = data.frame(x = c(0, h)),
  contrasts = cbind("AME x" = c(-1 / h, 1 / h)),
  CI = 0.95, CIType = "ETI", dpar = "sigma",
  effects = "fixedonly")
data.frame(bm$ContrastSummary)

mfx <- marginaleffects(ls.fe, dpar = "sigma", re_formula = NA)
summary(mfx)
```

# `effects`

The [`effects` package](https://cran.r-project.org/eb/packages/effects/index.html) was created by John Fox and colleagues.

+ `marginaleffects` supports 30+ more model types than `effects`.
+ `effects` focuses on the computation of ["adjusted predictions."](https://vincentarelbundock.github.io/marginaleffects/articles/predictions.html) The plots it produces are roughly equivalent to the ones produced by the `plot_cap` and `predictions` functions in `marginaleffects`.
+ `effects` does not appear support marginal effects (slopes), marginal means, or contrasts 
+ `effects` uses Base graphics whereas `marginaleffects` uses `ggplot2`
+ `effects` includes *a lot* of very powerful options to customize plots. In contrast, `marginaleffects` produces objects which can be customized by chaining `ggplot2` functions. Users can also call `plot_cap(model, draw=FALSE)` to create a prediction grid, and then work the raw data directly to create the plot they need

`effects` offers several options which are not currently available in `marginaleffects`, including:

* Partial residuals plots
* Many types of ways to plot adjusted predictions: [package vignette](https://cran.r-project.org/web/packages/effects/vignettes/predictor-effects-gallery.pdf)

# `modelbased`

The [`modelbased` package](https://easystats.github.io/modelbased/) is developed by the `easystats` team.

This section is incomplete; contributions are welcome.

* Wrapper around `emmeans` to compute marginal means and marginal effects.
* Powerful functions to create beautiful plots.

# `ggeffects`

The [`ggeffects`](https://strengejacke.github.io/ggeffects/) package is developed by Daniel LÃ¼decke.

This section is incomplete; contributions are welcome.

* Wrapper around `emmeans` to compute marginal means.

