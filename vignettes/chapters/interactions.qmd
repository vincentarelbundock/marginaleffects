---
bibliography: library.bib
---

```{r}
#| cache: false
#| echo: false
source("code/load.R")
```

# Interactions, polynomials, and splines {#sec-interactions}

So far, the models that we have considered were relatively simple. In this chapter, we apply the same workflow, framework, and software introduced in Parts I and II of the book to interpret estimates from sligthly more complex specifications. Our goal is to address two related issues: heterogeneity and flexibility.

Heterogeneity is present in virtually all empirical domains, when the effect of an intervention is stronger in some groups or contexts. For instance, a new treatment might significantly reduce blood pressure in younger adults, but have a weaker effect on older ones. Or a marketing campaign may increase sales in rural areas but not urban ones. This chapter shows how to use `marginaleffects` to report strata-specific effects, gauge if the impact of a variable is moderated by another, and gain a deeper understanding of context conditionality.

Flexible statistical models can be useful when studying the complex (potentially non-linear) relationships which are common in many fields.^[One downside of estimating more flexible models is that they may overfit the data. See TODO.] In environmental science, for example, the relationship between temperature and crop yield is often non-linear: as temperatures rise, crop yields may initially increase due to optimal growing conditions but eventually decline as heat stress becomes detrimental. This chapter shows how the same toolkit used to study heterogeneity can be leveraged to develop insight into complex empirical patterns.

We will focus on three strategies to account for heterogeneity and increase the flexibility of our models: multiplicative interactions, polynomials, and splines.

## Multiplicative interactions

We say that there is heterogeneity when the strength of the association between an explanator $X$ and an outcome $Y$ varies based on the value of a moderator $M$. Typically, $M$ is a variable which measures contextual elements, or characteristics of the individuals, groups, or units under observation. The key characteristic of a moderator is that it modifies the nature of the relationship between two other variables. $M$ can strengthen, weaken, or even reverse the association between an independent variable $X$ and the dependent variable $Y$.

One common strategy to study moderation is to fit a statistical model with multiplicative interactions [@BraClaGol2006; @KamFra2009; @ClaGol2023]. This involves creating a new composite variable by multiplying the explanator ($X$) to the moderator ($M$). When this composite variable is included in the model specification, its associated coefficient will capture $M$'s role in effect modification. A popular specification for moderation analysis is this linear model:^[In most cases, it is important to include all constitutive terms in addition to interactions. For example, if a model includes a multiplication between three variables $X\cdot W \cdot Z$, one would typically want to also include $X\cdot W, X\cdot Z, W\cdot Z, X, W,$ and $Z$. See @ClaGol2023 for details.]

$$Y = \beta_1 + \beta_2 \cdot M + \beta_3 \cdot X + \beta_4 \cdot X \cdot  M + \varepsilon,$$

where $Y$ is the outcome, $X$ is the cause of interest, and $M$ is a contextual variable which moderates the relationship between $X$ and $Y$. In that model, $\beta_3$ characterizes the association between $X$ and $Y$ when $M=0$. If the moderator $M$ is non-zero, the strength of association between $X$ and $Y$ is represented by more than the simple $\beta_3$ coefficient. We see this by inspecting the partial derivative:

$$\frac{\partial Y}{\partial X} = \beta_3 + \beta_4 \cdot M$$

This equation represents the slope of $Y$ with respect to $X$, that is, the extent to which $Y$ is expected to change in response to a small change in $X$. Clearly, this relationship now depends on more than just $X$ itself: the strength of association between $X$ on $Y$ is driven in part by the value of $M$. Thusly, this model specification allows the effect of an explanator to vary based on the value of a mediator.

In the next few sections, we will illustrate how to model heterogeneity using multiplicative interactions and how to interpret our parameter estimates using the software tools and conceptual framework in Parts I and II.

### Categorical-by-categorical

The first case to consider is when the association between a categorical explanator `X` and an outcome `Y` is moderated by a categorical variable `M`. This situation could occur when the effect of a binary treatment (treatment vs. control) on patient recovery (outcome) varies across different age groups (young, middle-aged, elderly).

To illustrate, we consider simulated data (@sec-interactions_simulation) with three variables: the outcome `Y` is a binary variable; the treatment `X` is a binary variable; and the moderator `M` is a categorical variable with 3 levels (a, b, and c).

We load the data and fit a logistic regression model:^[Note that when we insert an expression like `X*M` in the formula syntax, we instruct `R` to create binary variables for every level of the `M` predictor, and also to interact each of those levels with the `X` variable. Under the hood, `R` will multiply treatment and moderator to create the appropriate composite variables (`X`$\times$`Mb` and `X`$\times$`Mc`), and it will automatically omit one reference category to avoid perfect collinearity.]

```{r} 
library(marginaleffects)
library(modelsummary)
library(ggplot2)
library(patchwork)
dat <- read.csv("data/interaction_01.csv")
mod <- glm(Y ~ X * M, data = dat, family = binomial)
```


As is the case for many of the more complex models that we will consider, the coefficient estimates for this logit model with interactions are difficult to interpret on their own:

```{r} 
modelsummary(mod, coef_rename = TRUE, gof_map = "nobs")
```

Thankfully, we can rely on the framework and tools introduced in Parts I and II of this book to make these results intelligible.

#### Marginal predictions {.unnumbered}

Our first cut is to compute the average predicted outcome for each combination of `X` and `M`. As explained in @sec-predictions, this is equivalent to computing fitted values for every row in the original data, and then aggregating those fitted values by subgroups.^[An alternative would be to compute predictions on a different grid (ex: "balanced") before aggregating. This could be achieved by calling: `avg_predictions(mod, newdata="balanced", by=c("X","M"))`]

```{r}
avg_predictions(mod, by = c("X", "M"))
```
```{r}
#| echo: false
p <- avg_predictions(mod, by = c("X", "M"))
pmin <- sprintf("%.0f%%", min(p$estimate) * 100)
pmax <- sprintf("%.0f%%", max(p$estimate) * 100)
```

There is considerable variation in the predicted value of `Y` across subgroups, going from `{r} pmin` to `{r} pmax`. This variation can be made starker by plotting our results with the `plot_predictions()` function:

```{r}
#| label: fig-interactions_predictions_bin_cat
#| fig-cap: Average predicted probability that Y=1 for different values of M and X.
plot_predictions(mod, by = c("M", "X"))
```

@fig-interactions_predictions_bin_cat shows that, on average, the predicted probability that `Y=1` is considerably higher when `X=1`. Moreover, the difference in predicted outcomes $P(Y=1|X=1,M=a)-P(Y=1|X=0,M=a)$ seems smaller than $P(Y=1|X=1,M=c)-P(Y=1|X=0,M=c)$. This is a hint that we may want to formally check for effect moderation.

#### Does `X` affect `Y`? {.unnumbered}

We can build on these preliminary findings by adopting a more explicitly counterfactual approach, using the `comparisons()` family of function. Recall, from @sec-comparisons, that we can compute an average counterfactual comparison by following these steps:

1. Modify the original dataset by fixing `X` to 0 for all observations, and compute predictions for every row.
2. Modify the original dataset by fixing `X` to 1 for all observations, and compute predictions for every row.
3. Calculate the average difference between counterfactual predictions computed in steps 1 and 2.

These three steps can be taken with a single line of code:

```{r}
avg_comparisons(mod, variables = "X")
```
```{r}
#| echo: false
p <- avg_comparisons(mod, variables = "X")
pp <- sprintf("%.3f", p$estimate[1])
ppt <- sprintf("%.1f", p$estimate[1] * 100)
```

On average, moving from 0 to 1 on the `X` variable is associated with an increase of `{r} pp` on the outcome scale. Since we fit a logistic regression model, predictions are expressed on the probability scale. Thus, the estimate printed above suggests that the average treatment effect of `X` on `Y` is about `{r} ppt` percentage points. This estimate is statistically distinguishable from zero, as the small $p$ value attests.

#### Is the effect of `X` on `Y` moderated by `M`? {.unnumbered}

Now we can dive deeper, exploiting the multiplicative interaction in our model to interrogate heterogeneity. To see if the effect of `X` on `Y` depends on `M`, we make the same function call as above, but add the `by` argument: 

```{r}
avg_comparisons(mod, variables = "X", by = "M")
```
```{r}
#| echo: false
cmp <- avg_comparisons(mod, variables = "X", by = "M")
cmpA <- sprintf("%.0f", cmp$estimate[cmp$M == "a"] * 100)
cmpB <- sprintf("%.0f", cmp$estimate[cmp$M == "b"] * 100)
cmpC <- sprintf("%.0f", cmp$estimate[cmp$M == "c"] * 100)
```

On average, moving from the control (`X=0`) to the treatment group (`X=1`) is associated with an increase of `{r} cmpA` percentage points for individuals in category A. The average estimated effect of `X` for individuals in category C is `{r} cmpC`.

At first glance, these two estimated effects look different. But is the difference between `{r} cmpA` and `{r} cmpC` percentage points statistically significant? To answer this question, we can use the `hypothesis` argument and conduct a test of equality between the 1st and the 3rd estimate:

```{r} 
avg_comparisons(mod, variables = "X", by = "M", hypothesis = "b3 - b1 = 0")
```
```{r} 
#| echo: false
d <- avg_comparisons(mod, variables = "X", by = "M")
b3 <- sprintf("%.4f", d$estimate[3])
b1 <- sprintf("%.4f", d$estimate[1])
b3b1 <- sprintf("%.4f", d$estimate[3] - d$estimate[1])
```

The difference between the average estimated effect of `X` in categories C and A is: $`r b3` - `r b1` = `r b3b1`$. This difference is associated to a large $z$ statistic and a small $p$ value. Therefore, we can conclude that the difference is statistically significant; we can reject the null hypothesis that the effect of `X` is the same in sub-populations A and C.

### Categorical-by-continuous 

The second case to consider is an interaction between a categorical explanator (`X`) and a continuous mediator (`M`). To illustrate, we fit a new model to simulated data (see @sec-interactions_simulation):

```{r}
dat <- read.csv("data/interaction_02.csv")
mod <- glm(Y ~ X * M, data = dat, family = binomial)
modelsummary(mod, coef_rename = TRUE, gof_map = "nobs")
```

#### Conditional predictions {.unnumbered}

In the previous section, we started by compute average predictions for each combination of the interacted variable. When one of the variables is continuous and takes on many values (like `M`), it is not practical to report averages for every combination of `X` and `M`. Therefore, we focus on "conditional" estimates, obtained by calling the `predictions()` function. We use the `datagrid()` and `fivenum()` functions to create a grid of predictors based on Tukey's five number summary of `M`:^[These five numbers correspond to elements of a standard boxplot: minimum, lower-hinge, median, upper-hinge, and maximum.]

```{r} 
predictions(mod, newdata = datagrid(X = c(0, 1), M = fivenum))
```
```{r} 
#| echo: false
p <- predictions(mod, newdata = datagrid(X = c(0, 1), M = fivenum))
prmin <- sprintf("%.1f", min(p$estimate))
prmax <- sprintf("%.1f", max(p$estimate))
```

The results show considerable variation in the predicted $Pr(Y=1)$, ranging from `{r} prmin` to `{r} prmax`.

Instead of making predictions for discrete values of the continuous moderator `M`, we can also draw a plot with that variable on the x-axis:

```{r} 
#| label: fig-interactions-plot_predictions_num_bin
#| fig-cap: Predicted probability that Y=1, for different values of X and M
plot_predictions(mod, condition = c("M", "X"))
```

@fig-interactions-plot_predictions_num_bin shows that predicted values of `Y` tend to be lower when `M` is large. That figure also suggests that the relationship between `X` and `Y` has a different character for different values of $M$. When $M$ is small, we see $P(Y=1|X=1,M)<P(Y=1|X=0,M)$. When $M$ is large, the converse seems true.

#### Does `X` affect `Y`? {.unnumbered}

Moving to the counterfactual analysis, we call `avg_comparisons()` to get an overall estimate of the effect of `X` on the predicted $Pr(Y=1)$:

```{r}
avg_comparisons(mod, variables = "X")
```
```{r}
#| echo: false
cmp <- avg_comparisons(mod, variables = "X")
cmp <- sprintf("%.1f", abs(cmp$estimate) * 100)
```

On average, moving from 0 to 1 on `X` increases the predicted probability that `Y=1` by `{r} cmp[1]` percentage points.

#### Is the effect of `X` on `Y` moderated by `M`? {.unnumbered}

As explained in @sec-comparisons, we can estimate the effect of `X` for different values of `M` by using the `newdata` argument and `datagrid()` function. Here, we measure the strength of association between `X` and `Y` for two different values of `M`: its minimum and maximum.

```{r} 
comparisons(mod, variables = "X", newdata = datagrid(M = range))
```
```{r} 
#| echo: false
cmp <- comparisons(mod, variables = "X", newdata = datagrid(M = range))
cmp0 <- sprintf("%.3f", cmp$estimate[cmp$M == min(cmp$M)])
cmp1 <- sprintf("%.3f", cmp$estimate[cmp$M == max(cmp$M)])
```

Moving from 0 to 1 on the `X` variable is associated with a change of `{r} cmp0` in the predicted `Y` when the moderator `M` is at its minimum. Moving from 0 to 1 on the `X` variable is associated with a change of `{r} cmp1` in the predicted `Y` when the moderator `M` is at its maximum. Both of these estimates are associated with small $p$ values, so we can reject the null hypotheses that they are equal to zero.

Both estimates are different from zero, but are they different from one another? Is the effect of `X` on `Y` different when `M` takes on different values? To check this, we can add the `hypothesis` argument to the previous call:

```{r} 
comparisons(mod, 
  hypothesis = "b2 - b1 = 0",
  variables = "X",
  newdata = datagrid(M = range))
```

This confirms that the estimates are statistically distinguishable. We can reject the null hypothesis that `M` has no moderating effect the relationship between `X` and `Y`.

### Continuous-by-continuous

The third case to consider is an interaction between two continuous numeric variables: `X` and `M`. To illustrate, we fit a new model to simulated data (see @sec-interactions_simulation): 

```{r} 
dat <- read.csv("data/interaction_03.csv")
mod <- glm(Y ~ X * M, data = dat, family = binomial)
```

#### Conditional predictions {.unnumbered}

As in the previous cases, we begin by computing the predicted outcomes for different values of the predictors. In practice, the analyst should report predictions for predictor values that are meaningful to the domain of application. Here, we hold `X` and `M` to fixed arbitrary values:

```{r}
predictions(mod, newdata = datagrid(X = c(-2, 2), M = c(-1, 0, 1)))
```

Rather than focus on arbitrary point estimates, we can plot predicted values to communicate a richer set of estimates. When calling `plot_predictions()` with these data, we obtain a plot of predicted outcomes with the primary variable of interest (`X`) on the x-axis, and different lines representing different values of the moderator (`M`).

```{r} 
#| fig-cap: Predicted value of Y for different values of X and M
#| label: fig-interactions_plot_predictions_num_num
plot_predictions(mod, condition = c("X", "M"))
```

We can draw two preliminary conclusions from @fig-interactions_plot_predictions_num_num. First, the predicted values of `Y` depend strongly on the value of `X`. Moving from left to right in the plot often has a strong effect on the heights of predicted probability curves. Second, `M` strongly moderates the relationship between `X` and `Y`. Indeed, for some values of `M` the relationship of interest completely flips. For example, when `M` is around -3, the relationship between `X` and `Y` is negative: an increase in `X` is associated with a decrease in `Pr(Y=1)`. However, for all the other values of `M` that we considered, the relationship between `X` and `Y` is positive: an increase in `X` is associated withn an increase in `Pr(Y=1)`.

#### Does `X` affect `Y`? {.unnumbered}

To measure the "effect" of `X` on the predicted outcome, we can compute the average slope with respect to our predictor of interest:

```{r}
avg_slopes(mod, variables = "X")
```
```{r}
#| echo: false
s <- avg_slopes(mod, variables = "X")
s <- sprintf("%.3f", s$estimate)
```

On average, across all observed values of the moderator `M`, increasing `X` by one unit increases the predicted outcome by `{r} s`.^[Recall, from @sec-slopes, that this interpretation is valid for small changes in the neighborhoods where slopes are evaluated.] This is interesting, but as suggested by @fig-interactions_plot_predictions_num_num, there is strong heterogeneity in the relationship of interest, as a function of moderator `M`. Indeed, this is what we observe by computing slopes for different values of `M`:

#### Is the effect of `X` on `Y` moderated by `M`? {.unnumbered}

To answer this question, we estimate slopes of `Y` with respect to `X`, for different values of the moderator `M`: 

```{r} 
slopes(mod, variables = "X", newdata = datagrid(M = fivenum))
```

The results from this command confirm the intuition we developed based on @fig-interactions_plot_predictions_num_num. When `M` is strongly negative (-3), the slope is negative: increasing `X` results in a reduction of `Y`. However, for the other 4 values of `M` we consider, the slope is positive. This is consistent with @fig-interactions_plot_predictions_num_num, which shows one line with downward slope, and four lines with upward slopes.

For a more fine grained analysis, we can plot the slope of `Y` with respect to `X` for all observed values of the moderator `M`:

```{r}
#| fig-cap: Slope of `Y` with respect to `X`, for different values of the moderator `M`.
#| label: fig-interactions_plot_slopes_num_num
plot_slopes(mod, variables = "X", condition = "M") +
    geom_hline(yintercept = 0, linetype = "dotted")
```

@fig-interactions_plot_slopes_num_num plot shows that when the moderator `M` is below -1, the relationship between `X` and `Y` is negative: increasing `X` decreases `Y`. However, when `M` rises to about -1, the relationship between `X` and `Y` becomes positive: increasing `X` increases `Y`.

We can confirm that this moderation effect is statistically significant using the `hypothesis` argument:

```{r} 
slopes(mod,
  hypothesis = "b2 - b1 = 0", 
  variables = "X",
  newdata = datagrid(M = range))
```

The $\frac{\partial Y}{\partial X}$ slope is larger when evaluated at maximum `M`, than at minimum `M`. Therefore, we can reject the null hypothesis that `M` has no moderating effect on the relationship between `X` and `Y`.

### Multiple interactions

The fourth case to consider is when more than two variables are included in multiplicative interactions. Such models have serious downsides: they can overfit the data, and they impose major costs in terms of statistical power, typically requiring considerably larger sample sizes than models without interaction. On the upside, models with multiple interactions allow more flexibility in modelling, and they can capture complex patterns of moderation between regressors.

Models with several multiplicative interactions do not pose any particular interpretation challenge, since the tools and workflows introduced in this book can be applied to these models in straightforward fashion. Consider this model, fit to simulated data, with three binary variables multipled to each other:

```{r} 
dat <- read.csv("data/interaction_04.csv")
mod <- glm(Y ~ X * M1 * M2, data = dat, family = binomial)
modelsummary(mod, coef_rename = TRUE, gof_map = "nobs")
```

Once again, the coefficient estimates of this logistic regression are difficult to interpret on their own, so we use functions from the `marginaleffects` package. 

#### Marginal predictions {.unnumbered}

As before, we can compute and display marginal predicted outcomes in any subgroup of interest, using the `avg_predictions()` or `plot_predictions()`:

```{r}
#| label: fig-interactions_plot_predictions_multiple
#| fig-cap: Average predicted outcomes for different combinations of `X`, `M1`, and `M2`.
#| out-width: 100%
#| fig-width: 8.571429
#| fig-asp: .4
plot_predictions(mod, by = c("X", "M1", "M2"))
```

#### Does `X` affect `Y`? {.unnumbered}

As before, we can estimate the average change in `Y` associated with a change from 0 to 1 in the `X` variable using the `avg_comparisons()` function:

```{r} 
avg_comparisons(mod, variables = "X")
```
```{r} 
#| echo: false
cmp <- avg_comparisons(mod, variables = "X")$estimate
cmp <- sprintf("%.1f", cmp * 100)
```

This suggests that, on average, moving from the control (0) to the treatment (1) group is associated with an increase of `{r} cmp` percentage points in the probability that `Y` equals 1. The $p$ value is small, which implies that we can reject the null hypothesis that `X` has no effect on the predicted outcome.

#### Is the effect of `X` on `Y` moderated by `M1`? {.unnumbered}

We can also estimate how the effect of `X` varies based on different values of moderator `M1`:

```{r} 
avg_comparisons(mod, variables = "X", by = "M1")
```
```{r} 
#| echo: false
cmp <- avg_comparisons(mod, variables = "X", by = "M1")
cmp <- sprintf("%.4f", cmp$estimate)
```

The results suggest that the change in `Y` associated with a change in `X` differs based on the value of `M1`: `{r} cmp[1]` vs. `{r} cmp[2]`. By using the `hypothesis` argument, we can confirm that the difference between these two estimated effect sizes is statistically significant:

```{r} 
avg_comparisons(mod, 
  variables = "X",
  by = "M1",
  hypothesis = "b2 - b1 = 0")
```

#### Does the moderating effect of `M1` depend on `M2`? {.unnumbered}

The last question that we pose is more complex. Above, we established that:

1. On average, `X` affects the predicted value of `Y`.
2. On average, the value of `M1` modifies the strength of association between `X` and `Y`.

Now we ask if `M2` changes the way in which `M1` moderates the effect of `X` on `Y`. The difference is subtle but important: we are asking if the moderation effect of `M1` is itself moderated by `M2`.

The following code computes the average difference in predicted `Y` associated with a change in `X`, for every combination of moderators `M1` and `M2`. Each row represents the average effect of `X` at different points in the sample space:

```{r} 
avg_comparisons(mod, 
    variables = "X", 
    by = c("M2", "M1"))
```
```{r} 
#| echo: false
cmp <- avg_comparisons(mod, 
    variables = "X", 
    by = c("M2", "M1"))
cmp00 <- sprintf("%.1f", cmp$estimate[cmp$M1 == 0 & cmp$M2 == 0] * 100)
cmp01 <- sprintf("%.1f", cmp$estimate[cmp$M1 == 0 & cmp$M2 == 1] * 100)
```

When we hold the moderators fixed at `M1=0` and `M2=0`, changing the value of `X` from 0 to 1 changes the predicted value of `Y` by `{r} cmp00` percentage points. When we hold the moderators fixed at `M1=0` and `M2=1`, changing the value of `X` from 0 to 1 changes the predicted value of `Y` by `{r} cmp01` percentage points.

Now, imagine that we hold `M2` constant at 0. We can determine if the effect of `X` is moderated by `M1` by using the `hypothesis` argument to compare estimates in rows 1 and 2. This shows that the estimated effect size of `X` is larger when `M1=1` than when `M1=0`, holding `M2` at 0.

```{r} 
avg_comparisons(mod, 
    hypothesis = "b2 - b1 = 0",
    variables = "X", 
    by = c("M2", "M1"))
```

Similarly, imagine that we hold `M2` constant at 1. We can determine if the effect of `X` is moderated by `M1` by comparing estimates in rows 3 and 4. This hypothesis test shows that the effect size of `X` is larger when `M1=1` than when `M1=0`, holding `M2` at 1.

```{r} 
avg_comparisons(mod, 
    hypothesis = "b4 - b3 = 0",
    variables = "X", 
    by = c("M2", "M1"))
```

The last two estimates can be interpreted as measuring the extent to which `M1` acts as a moderator, holding `M2` at different values. To answer the question of whether `M2` moderates the moderation effect of `M1`, we can specify the `hypothesis` as a difference in differences:

```{r} 
avg_comparisons(mod, 
    hypothesis = "(b2 - b1) - (b4 - b3) = 0",
    variables = "X", 
    by = c("M2", "M1"))
```
```{r} 
#| echo: false
pv <- avg_comparisons(mod, 
    hypothesis = "(b2 - b1) - (b4 - b3) = 0",
    variables = "X", 
    by = c("M2", "M1"))
pv <- sprintf("%.4f", pv$p.value)
```

This suggests that `M2` *may* have a second order moderation effect, but we cannot completely completely rule out the null hypothesis because the $p$ value does not cross conventional thresholds of statistical significance ($p$=`{r} pv`).

### Hypothesis tests: effect vs. moderation

TODO: Two research questions are often conflated:

1. Effect: Does $X$ affect $Y$?
    - Is the $\frac{\partial Y}{\partial X}$ slope different from 0 when $M=m$?
2. Moderation: Does $M$ moderate the relationship between $X$ and $Y$?
   - Is the $\frac{\partial Y}{\partial X}$ different when $M=m$ and $M=n$?

## Polynomial regression

Polynomial regression is an extension of linear regression that allows for modeling the relationship between a dependent variable $Y$ and an independent variable $X$ as an nth-degree polynomial. While the model specification remains linear in the coefficients, it is polynomial in the value of $X$. This type of regression is useful when the data shows a non-linear relationship that a straight line cannot adequately capture.

The general form of a polynomial regression model is:

$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \cdots + \beta_n X^n + \varepsilon,$$

where $Y$ is the dependent variable, $X$ is the independent variable, $\beta_0, \beta_1, \beta_2, \ldots, \beta_n$ are the coefficients to be estimated, $n$ is the degree of the polynomial, and $\varepsilon$ represents the error term. For instance, a second-degree (quadratic) polynomial regression equation can be written as:

$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \varepsilon$$

This can be treated as a linear regression problem by constructing new variables $Z_1=X$, $Z_2=X^2$, etc. The model then becomes $Y=\beta_0+\beta_1\cdot Z_1+\beta_2\cdot Z_2+\varepsilon$, which can be estimated using standard methods like ordinary least squares.


Polynomial regression offers several key advantages, particularly in its flexibility and ability to fit a wide range of curves, simply by adjusting the degree of the polynomial. As a result, polynomial regression can reveal underlying patterns in the data that are not immediately apparent with simpler models.

This approach also has notable disadvantages. One significant issue is its potential for overfitting, especially when the $n$ order is high. Moreover, polynomial regression can suffer from unreliable extrapolation, where predictions made outside the range of the training data can become erratic and unrealistic. Consequently, while polynomial regression can be powerful, careful consideration must be given to the degree of the polynomial to balance fit and generalization effectively.

Polynomial regression can be viewed simply as a model specification with several variables interacted with themselves. As such, it can be interpreted using exactly the same tools discussed in the earlier part of this chapter. To illustrate, we consider two simple data generating processes adapted from @HaiMumXu2019. The first is:

\begin{align*}
Y = 2.5 - X^2 + \nu, && \text{where }\nu\sim N(0,1),X\sim U(-3,3)
\end{align*}

If we fit a linear model with only $X$ as predictor, the line of best fit will not be a good representation of the data. However, a cubic polynomial regression can easily detect the curvilinear relationship between $X$ and $Y$. In `R` and `Python`, we can use similar syntax to specify polynomials directly in the model formula:^[For `marginaleffects` to work properly in this context, it is important to specify the polynomials in the model-fitting formula. Users should not hard-code the values by creating new variables in the dataset before fitting the model.]

```{r} 
#| label: fig-interactions_polynomials
#| fig-cap: "Modelling a curvilinear relationship with linear or polynomial regression."
#| out-width: 100%
#| fig-width: 8.571429
#| fig-asp: .4
library(patchwork)
dat <- read.csv("data/polynomial_01.csv")
mod_linear <- lm(Y ~ X, data = dat)
mod_cubic <- lm(Y ~ X + I(X^2) + I(X^3), data = dat)
p1 <- plot_predictions(mod_linear, condition = "X", points = .05) + ggtitle("Linear")
p2 <- plot_predictions(mod_cubic, condition = "X", points = .05) + ggtitle("Cubic")
p1 + p2
```
Clearly, the model with polynomials makes much better predictions, ones that capture the curvilinear relationship between $X$ and $Y$. We can now evaluate the strength of association between $X$ and $Y$ by computing the slope of the outcome equation with respect to $X$, for different values of $X$:

```{r}
slopes(mod_cubic, variables = "X", newdata = datagrid(X = c(-2, 0, 2)))
```

When $X$ is negative (-2), the slope is positive which indicates that an increase of $X$ is associated with an increase in $Y$. When $X$ is around 0, the slope is null, which indicates that the strength of association between $X$ and $Y$ is null (or very weak). When $X$ is around 0, changing $X$ by a small amount will have almost no effect on $Y$. When $X$ is positive (-2), the slope is negative. This indicates that increasing $X$ will result in a decrease in $Y$.

Now, consider a sligthly different data generating process, where a binary moderator $D$ changes the nature of the relationship between $X$ and $Y$:^[This data generating process is adapted from @HaiMumXu2019.]

\begin{align*}
Y = 2.5 - X^2 - 5 \cdot M + 2 \cdot M \cdot X^2 + \nu &&  \text{where }\nu\sim N(0,1),X\sim U(-3,3)
\end{align*}

If we simply fit a cubic regression, without accounting for $M$, our predictions will be inaccurate. However, if we interact the moderator $M$ with all polynomial terms (using parentheses as a shortcut for the distributive property), we can get an excellent fit for the curvilinear and differentiated relationship between $X$ and $Y$:

```{r}
#| out-width: 100%
#| fig-width: 8.571429
#| fig-asp: .4
dat <- read.csv("data/polynomial_02.csv")

# cubic
mod_cubic <- lm(Y ~ X + I(X^2) + I(X^3), data = dat)

# cubic + interaction
mod_cubic_int <- lm(Y ~ M * (X + I(X^2) + I(X^3)), data = dat)

p1 <- plot_predictions(mod_cubic, condition = "X", points = .05)
p2 <- plot_predictions(mod_cubic_int, condition = c("X", "M"), points = .1)
p1 + p2
```

Of course, we can also estimate the slope of the outcome equation for different values of `M` and `X`:

```{r}
slopes(mod_cubic_int,
  variables = "X",
  newdata = datagrid(M = c(0, 1), X = fivenum))
```

## Splines

TODO:

* What is a spline?
* What is a GAM?
    - Don't do `avg_comparisons()`: https://stats.stackexchange.com/questions/654784/how-to-decide-which-spline-to-use-when-conducting-g-computation-after-weighting/654791#654791

```{r}
#| message: false
library(mgcv)

# mgcv::gam should treat M as categorical (factor)
mod <- gam(Y ~ s(X, by = factor(M)) + M, data = dat)

plot_predictions(mod, condition = c("X", "M"), points = .1)
```

```{r} 
slopes(mod, variables = "X", newdata = datagrid(M = c(0, 1), X = c(-1, 0, 1)))
```

## Data simulation {#sec-interactions_simulation}

This code generates the simulated datasets used in this chapter:

```{r} 
#| eval: false
# Multiplicative interactions: X x M
set.seed(1024)
N <- 5000
X <- rbinom(N, 1, .5)
M <- sample(c("a", "b", "c"), N, replace = TRUE)
b <- runif(8, -1, 1)
Y <- rbinom(N, 1, prob = plogis(
  b[1] + b[2] * X +
  b[3] * (M == "b") + b[4] * (M == "b") + b[5] * (M == "c") +
  b[6] * X * (M == "a") + b[7] * X + (M == "b") + 
  b[8] * X * (M == "c")
))
dat <- data.frame(Y, X, M)
write.csv(dat, "data/interaction_01.csv")
```

```{r} 
#| eval: false
# Multiplicative interactions: X x M
set.seed(1024)
N <- 5000
X <- rbinom(N, 1, .5)
M <- rnorm(N)
b <- runif(4, -1, 1)
Y <- rbinom(N, 1, prob = plogis(
  b[1] + b[2] * X + b[3] * M + b[4] * X * M
))
dat <- data.frame(Y, X, M)
write.csv(dat, "data/interaction_02.csv")
```

```{r}
#| eval: false
# Multiplicative interactions: X x M
set.seed(1024)
N <- 5000
X <- rnorm(N)
M <- rnorm(N)
b <- runif(4, -1, 1)
Y <- rbinom(N, 1, prob = plogis(
  b[1] + b[2] * X + b[3] * M + b[4] * X * M
))
dat <- data.frame(Y, X, M)
write.csv(dat, "data/interaction_03.csv")
```

```{r}
#| eval: false
# Multiplicative interactions: X x M1 x M2
set.seed(1024)
N <- 5000
X <- rbinom(N, 1, .5)
M1 <- rbinom(N, 1, .5)
M2 <- rbinom(N, 1, .5)
b <- runif(8, -1, 1)
Y <- rbinom(N, 1, prob = plogis(
  b[1] +
  b[2] * X + b[3] * M1 + b[4] * M2 + 
  b[5] * X * M1 + b[6] * X * M2 + b[7] * M1 * M2 + 
  b[8] * X * M1 * M2
))
dat <- data.frame(Y, X, M1, M2)
write.csv(dat, "data/interaction_04.csv")
```

```{r}
#| eval: false
# Polynomial regression: X and Y
set.seed(1024)
N <- 1e3
FUN <- function(X) 2.5 - X^2
X <- runif(N, min = -3, max = 3)
Y <- FUN(X) + rnorm(N, sd = .5)
dat <- data.frame(X, Y)
write.csv(dat, "data/polynomial_01.csv")
```

```{r}
#| eval: false
# Polynomial regression: X, D, and Y
set.seed(1024)
N <- 1e3
X <- runif(N, min = -3, max = 3)
M <- rbinom(N, size = 1, prob = .5)
Y <- 2.5 - X^2 - 5 * M + 2 * M * X^2 + rnorm(N, .5)
dat <- data.frame(X, M, Y)
write.csv(dat, "data/polynomial_02.csv")
```

