
# Equivalence Tests


```{r, include = FALSE}
options(width = 1000)
## this vignette is in .Rbuildignore because lme4 is not available on old CRAN
## test machines.

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 9,
  fig.asp = .4,
  out.width = "100%",
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```


In many contexts, analysts are less interested in rejecting a null hypothesis, and more interested in testing whether an estimate is "inferior", "superior", or "equivalent" to a given threshold or interval. For example, medical researchers may wish to determine if the estimated effect of a new treatment is larger than the effect of prior treatments, or larger than some threshold of "clinical significance." Alternatively, researchers may wish to support a claim that an estimated parameter is "equivalent to" or "not meaningfully different from" a null hypothesis.

To answer these questions, we can use non-inferiority, non-superiority, or equivalence tests like the two-one-sided test (TOST). This article gives a primer and tutorial on TOST:

> Lakens D, Scheel AM, Isager PM. Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science. 2018;1(2):259-269. doi:10.1177/2515245918770963

The `hypotheses()` function of the `marginaleffects` package includes an `equivalence` argument which allows users to apply these tests to any of the quantities generated by the package, as well as to arbitrary functions of a model's parameters. To illustrate, we begin by estimating a simple linear regression model:

::: {.panel-tabset group="language"}
### R
```{r}
library(marginaleffects)
mod <- lm(mpg ~ hp + factor(gear), data = mtcars)
```
### Python
```{python}
import polars as pl
import statsmodels.formula.api as smf
from marginaleffects import *

mtcars = pl.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv")
mtcars = mtcars.cast({"gear" : pl.Utf8, "hp" : pl.Float64})

mod = smf.ols("mpg ~ hp + gear", data = mtcars.to_pandas()).fit()
```
:::

The rest of this section considers several quantities estimated by `marginaleffects`.

## Predictions

Consider a single prediction, where all predictors are held at their median or mode:

::: {.panel-tabset group="language"}
### R
```{r}
p <- predictions(mod, newdata = "median")
p
```
### Python
```{python}
p = predictions(mod, newdata = "median")
print(p)
```
:::
```{r, include = FALSE}
options(width = 120)
hyp <- hypotheses(p, equivalence = c(17, 18))
```

Now we specify an equivalence interval (or "region") for predictions between 17 and 18:

::: {.panel-tabset group="language"}
### R
```{r}
hypotheses(p, equivalence = c(17, 18))
```
### Python
```{python}
# Not implemented yet
```
:::

The results allow us to draw three conclusions:

1. The p value for the non-inferiority test is `r sprintf("%.4f", hyp$p.value.noninf)`. This suggests that we *can* reject the null hypothesis that the parameter is below 17.
1. The p value for the non-superiority test is `r sprintf("%.4f", hyp$p.value.nonsup)`. This suggests that we *cannot* reject the null hypothesis that the parameter (`r sprintf("%.4f", hyp$estimate)`) is above 18.
1. The p value for the equivalence test is `r sprintf("%.4f", hyp$p.value.equiv)`. This suggests that we *cannot* reject the hypothesis that the parameter falls outside the equivalence interval.

## Model coefficients

The `hypotheses` function also allows users to conduct equivalence, non-inferiority, and non-superiority tests for model coefficients, and for arbitrary functions of model coefficients.

Our estimate of the coefficient affecting `r names(coef(mod))[4]` in the model is:

::: {.panel-tabset group="language"}
### R
```{r}
coef(mod)[4]
```
### Python
```{python}
mod.params['gear[T.5]']
```
:::

We can test if this parameter is likely to fall in the [5,7] interval by:

::: {.panel-tabset group="language"}
### R
```{r}
hypotheses(mod, equivalence = c(5, 7))[4, ]
```
### Python
```{python}
h = pl.DataFrame(hypotheses(mod, equivalence = [5., 7.]))[2,:]
print(h)
```
:::
```{r, include = FALSE}
hyp <- hypotheses(mod, equivalence = c(5, 7))[4, "p.value.equiv"]
```

The p value is `r sprintf("%.4f", hyp)`, so we cannot reject the hypothesis that the `r names(coef(mod))[4]` parameter falls outside the [5,7] interval. 

## Slopes

The same syntax can be used to conduct tests for all the quantities produced by the `marginaleffects` package. For example, imagine that, for substantive or theoretical reasons, an average slope between -0.1 and 0.1 is uninteresting. We can conduct an equivalence test to check if this is the case:

::: {.panel-tabset group="language"}
### R
```{r}
avg_slopes(mod, variables = "hp", equivalence = c(-.1, .1))
```
### Python
```{python}
s = pl.DataFrame(avg_slopes(mod, variables = "hp", equivalence = [-.1, .1]))
print(s)
```
:::
```{r, include = FALSE}
hyp <- avg_slopes(mod, variables = "hp", equivalence = c(-.1, .1))$p.value.equiv
```

The p value is `r sprintf("%.4f", hyp)`, which suggests that we can reject the hypothesis that the parameter falls outside the region of "substantive equivalence" that we have defined by the interval.

## Difference between comparisons (contrasts)

Consider a model with a multiplicative interaction:

::: {.panel-tabset group="language"}
### R
```{r}
int <- lm(mpg ~ hp * factor(gear), data = mtcars)
```
### Python
```{python}
inter = smf.ols("mpg ~ hp * gear", data = mtcars.to_pandas()).fit()
```
:::

The average contrast for a change of 1 unit in `hp` differs based on the value of `gear`:

::: {.panel-tabset group="language"}
### R
```{r}
avg_comparisons(int, variables = "hp", by = "gear")
```
### Python
```{python}
print(avg_comparisons(inter, variables = "hp", by = "gear"))
```
:::

Are these contrasts different from one another? Let's look at the pairwise differences between them:

::: {.panel-tabset group="language"}
### R
```{r}
avg_comparisons(int, variables = "hp", by = "gear",
    hypothesis = "pairwise")
```
### Python
```{python}
print(avg_comparisons(inter, variables = "hp", by = "gear",
    hypothesis = "pairwise"))
```
:::

We consider that these pairwise comparisons are "equivalent to zero" when they fall in the [-.1, .1] interval:

::: {.panel-tabset group="language"}
### R
```{r}
avg_comparisons(int, variables = "hp", by = "gear",
    hypothesis = "pairwise",
    equivalence = c(-.1, .1))
```
### Python
```{python}
c = pl.DataFrame(
    avg_comparisons(inter, variables = "hp", by = "gear",
        hypothesis = "pairwise",
        equivalence = [-.1, .1])
    )
print(c)
```
:::

The `p (Equiv)` column shows that the difference between the average contrasts when `gear` is 3 and `gear` is 5 can be said to be equivalent to the specified interval. However, there are good reasons to think that the other two pairwise comparisons may fall outside the interval.

## Marginal means and `emmeans`

This example shows the equivalence between results produced by the `emmeans` package and the `predictions()` function:

```{r}
library(emmeans)

mod <- lm(log(conc) ~ source + factor(percent), data = pigs)

## {emmeans}
emmeans(mod, specs = "source") |>
    pairs() |>
    test(df = Inf,
         null = 0,
         delta = log(1.25),
         side = "equivalence",
         adjust = "none")

## {marginaleffects}
predictions(
    mod,
    newdata = "balanced",
    by = "source",
    hypothesis = "pairwise",
    equivalence = c(-log(1.25), log(1.25))
)
```


## t-test

Now we show that the results produced by `hypotheses()` are identical to the results produced by the `equivalence` package in the case of a simple t-test:

```{r}
library(equivalence)

set.seed(1024)

## simulate data data
N <- 20
dat <- data.frame(
    y = rnorm(N),
    x = sample(c(rep(0, N / 2), rep(1, N / 2)), N))

## fit model
mod <- lm(y ~ x, data = dat)

## test with the {equivalence} package
e <- tost(
    x = dat$y[dat$x == 0],
    y = dat$y[dat$x == 1],
    epsilon = 10)
e

## test with {marginaleffects} package
h <- hypotheses(mod, equivalence = c(-10, 10), df = e$parameter)[2, ]
h

# identical p values
h$p.value.equiv |> as.vector()

e$tost.p.value |> as.vector()
```
