---
title: Comparisons 
---

```{r, include = FALSE}
options(width = 1000)
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .4,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)

library(marginaleffects)
library(patchwork)
library(ggplot2)

theme_set(theme_minimal())
```

In this vignette, we introduce "comparisons", defined as:

> Comparisons are functions of two or more predictions. Example of comparisons include contrasts, differences, risks ratios, odds, etc.

The `comparisons()` function is *extremely* flexible, and it allows users to estimate a vast array of quantities of interest. To describe those quantities, we will break the problem down into in 4 steps:

1. Quantity
2. Grid
3. Aggregation
4. Uncertainty
5. Test

These steps can be combined, mixed, and matched to define and compute many different estimands. With comparisons, we can answer questions like these:

- _Medical Treatment Efficacy_: How does probability of survival differ for patients who receive a new medication relative to a placebo?
- _Education Policy_: How do standardized test scores differ between students who attended preschool programs and those who did not?
- _Environmental Conservation_: What is the impact of new conseration policies on forest coverage?
- _Economic Development_: What difference does participation in microfinance programs make on household income?
- _Sports Performance_: How does the win ratio of teams change after the introduction of a new training regimen?
- _Cultural Studies_: What is the difference in cultural engagement levels between communities with access to public libraries and those without?

Using the `hypothesis` argument of the `comparisons()` function, we will also be able to compare contrasts to one another, answering questions such as:

* Is the effect of an educational intervention on mathematical competency stronger for older or younger students?
* Does a work training program in a carceral setting have a greater effect on the probability of recidivism for first or second time offenders?

In this vignette, we will illustrate these concepts by focusing on variations on this question:

> How does the probability of survival (outcome) change if a passenger travels in 1st class vs. 3rd class?

```{r}
library(marginaleffects)

dat <- "https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv"
dat <- read.csv(dat, na.strings = c("*", ""))

mod <- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)

summary(mod)
```

These coefficient estimates are interesting, but difficult to interpret. In many applications, analysts will want to call the `comparisons()` or `avg_comparisons()` functions to report more meaningful quantities of interest. The rest of explains in detail how to interpret and customize calls like this one:

```{r}
avg_comparisons(mod)
```


# Quantity of interest

To investigate the effect of gender, age, and passenger class on survival aboard the Titanic, we will conduct a series of "comparisons" between predicted outcomes computed for different preditor values. The quantity of interest will vary in two main respects:

* *Predictor type*
    * How does the explanator of interest change?
    * Ex: +1 on a numeric variable, or the difference between a factor level and its baseline.
* *Comparison type*
    * How do we compare the two predicted outcomes?
    * Ex: difference, ratio, log odds, lift, etc.
* Outcome type
    * What is the scale of the response variable?
    * Ex: response, link, hazard, etc.

By default, the `comparisons()` function will estimate the "effect" of different changes for different types of predictors:

* Numeric: Increase of 1 unit.
* Logical: Change from `FALSE` to `TRUE`.
* Factor or character: Changes from the reference to each other level.


## Comparison type

The default comparison type in  `comparisons()` is the "difference", which means that the  `comparisons()` function will typically produce quantities which are often interpreted as a measure of "effect" in applications: risk differences, average treatment effects, G-computation estimates, etc. But `comparisons()` is not limited to differences, Indeed, it allows analysts to call on many built-in functions for comparison, and also to supply their own.

### Differences

By default, the `comparisons()` function will compare the predicted values by differencing (subtraction). For example, consider a hypothetical Titanic passenger with these characteristics:

```{r}
passenger <- data.frame(
    PClass = "3rd",
    Age = 20,
    SexCode = 0
)
```

What would happen to the predicted probability of survival if we were to increase the `SexCode` from 0 to 1? Or increase `Age` by 1 unit? Or change `PClass` from "3rd" to "1st"? To answer these questions, we could proceed manually by computing two predictions and then subtracting one from the other:

```{r}
passenger_0 <- passenger
passenger_1 <- transform(passenger, SexCode = 1)

prediction_0 = predict(mod, newdata = passenger_0, type = "response")
prediction_1 = predict(mod, newdata = passenger_1, type = "response")

prediction_1 - prediction_0
```

The result above shows that modifying the `SexCode` variable of this hypothetical passenger from 0 to 1 increases the predicted probability of survival by about `r sprintf("%.f", 100 * (prediction_1-prediction_0))` percentage points.

Instead of computing this estimate by hand, we could call `comparisons()` to get analogous results for all predictors in the model at once:

```{r}
comparisons(mod, newdata = passenger)
```

### Ratios

Instead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, [the `adjrr` function the `Stata` software package](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300304) can compute "adjusted risk ratios", which are ratios of adjusted predictions. To do this in `R`, we could use the same predictions we computed above:

```{r}
prediction_1 / prediction_0
```

This shows that the predicted probability of survival would be about `r sprintf("%.1f%%", (prediction_1/prediction_0))` larger if our hypothetical passenger were a woman than a man. 

The same result can be obtained by setting the `comparison` argument:

```{r}
comparisons(mod, comparison = "ratio", newdata = passenger)
```


### Built-in functions

Beyond differences and ratios, `comparisons()` supports many other built-in functions to compare predictions, such as log odds ratios, lift, etc. See `?comparisons` for a complete list.

These built-in functions allow us convenient access to complex transformations. For example, this code computes the log odds ratio associated with a change in predictors:

```{r}
comparisons(mod, 
    comparison = "lnor", 
    newdata = passenger)
```

### Custom functions


Analysts who need more flexibility can define their own comparison functions. For example, these two calls are equivalent:

```{r}
comparisons(mod, 
    comparison = function(hi, lo) hi / lo, 
    newdata = passenger)

comparisons(mod, 
    comparison = function(hi, lo) hi / lo, 
    newdata = passenger)
```

This mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:

```{r}
comparisons(mod,
    comparison = function(hi, lo) sqrt(hi) / log(lo + 10), 
    newdata = passenger)
```


## Predictor types

The types of comparisons that one might care about depend on the kinds of predictors in our model: numeric, binary, or categorical.


### Numeric

We can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the `Age` variable by 1 unit (default) or by 5 units:

```{r}
comparisons(mod, variables = "Age", newdata = passenger)

comparisons(mod, variables = list(Age = 5), newdata = passenger)
```

Compare adjusted predictions for a change in the regressor between two arbitrary values:

```{r}
comparisons(mod, variables = list(Age = c(5, 60)), newdata = passenger)
```

Compare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:

```{r}
comparisons(mod, variables = list(Age = "iqr"), newdata = passenger)

comparisons(mod, variables = list(Age = "sd"), newdata = passenger)

comparisons(mod, variables = list(Age = "2sd"), newdata = passenger)

comparisons(mod, variables = list(Age = "minmax"), newdata = passenger)
```


### Binary or Logical

For logical or binary variables, the default comparison correponds to a change from 0 to 1, or from `FALSE` to `TRUE`:

```{r}
dat2 <- transform(dat, SexCode = as.logical(SexCode))
mod2 <- glm(Survived ~ PClass * SexCode * Age, data = dat2, family = binomial)

comparisons(mod, variables = "SexCode", newdata = passenger)

comparisons(mod2, variables = "SexCode", newdata = passenger)
```

### Factor or Character


The `comparisons()` function automatically computes contrasts for each level of a categorical variable (factor or character), relative to the baseline category, while holding all other values at their observed values. We can obtain different contrasts by using the `variables` argument:

```{r}
comparisons(mod, variables = list(PClass = "sequential"), newdata = passenger)

comparisons(mod, variables = list(PClass = "pairwise"), newdata = passenger)

comparisons(mod, variables = list(PClass = "reference"), newdata = passenger)
```

We can also specify a particular contrast of interest in `variables`:

```{r}
comparisons(mod, 
    variables = list(PClass = c("3rd", "1st")),
    newdata = passenger)
```

### Cross-contrasts


In other contexts, we are interested in a "cross-contrast" or "cross-comparisons", that is, we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the `variables` argument, and set the `cross=TRUE`.

```{r}
cmp <- comparisons(mod, 
    variables = list(SexCode = 0:1, Age = 5),
    cross = TRUE,
    newdata = passenger)
cmp
```

This tells us that changing our hypothetical passenger from `SexCode=0` to 1, and from `Age=20` to 25 *simultaneously* is associated with a change of `r sprintf("%.2f", cmp$estimate[1])` in the predicted probability of survival (on a 0 to 1 scale).


## Outcome type

We can compute contrasts on different response scales. In GLM model, for example, we tend to estimate contrasts and comparisons on the "response" scale, becasue the results are expressed on the natural unit of measurement of the dependent variable. However, we an also compute the quantity on the "link" scale, by changing the `type` argument:

```{r}
comparisons(mod, type = "response", newdata = passenger)

comparisons(mod, type = "link", newdata = passenger)
```

The support `type` values depend on the kind of fitted model at hand. Supported types for a specific model are printed to screen when a bad type is entered:

```{r}
#| error: true
comparisons(mod, type = "bad type name")
```


# Grid 

In most statistical models, comparisons (differences, ratios, etc.) are *conditional* quantities, in the sense that they typically depend on the values of all the predictors in the model. Therefore, when we compute a comparison, we need to decide *where* to evaluate it in the predictor space. 

A "profile" is a combination of values of the predictor variables in a model. A "grid" is a collection of one or more profiles. You can think of a "grid" as different rows in a dataset, where each row contains the information necessary to predict the outcome for one individual or unit of observation.

A "profile" can be defined as a specific combination of predictor values, and a "grid" can be defined as a collection of profiles for which we can evaluate quantities of interest.

There are many possible grids:

* Empirical distribution
* User-specified values
* Representative values


## Empirical distribution

By default, the `comparisons()` function returns estimates for every single row of the original data frame which was used to fit the model. The Titanic dataset includes `r nobs(mod)` complete observations (after dropping missing data), so this command will yield `r nobs(mod)` estimates:

```{r}
comparisons(mod, variables = "Age")
```

If we do not specify the `variables` argument, `comparisons()` computes 4 distinct contrasts for all the variables, so we get $4\\times `r nobs(mod)`=`r 4 * nobs(mod)`$ rows:

```{r}
comparisons(mod)
```

We can plot the full distribution of unit-specific contrasts easily:

```{r}
#| warning: false
#| message: false
#| fig-asp: .3
library(ggplot2)

comparisons(mod) |>
    ggplot(aes(x = estimate)) +
    geom_histogram(bins = 30) +
    facet_grid(. ~ term + contrast, scales = "free")
```

The plot above suggests that there is substantial heterogeneity in treatment effects across different unit characteristics.


## User-specified values

In some contexts, it is interesting to estimate a contrast for a specific individual with characteristics of interest. To achieve this, we can supply a data frame to the `newdata` argument. 

This code shows the expected change in probability of survival in the counterfactual world where Mr Harry Anderson had been 20 years older:

```{r}
unit <- subset(dat, Name == "Anderson, Mr Harry")

comparisons(mod, newdata = unit)
```

A very convenient way to create grids of units with specific predictor values is to the `datagrid()` function from the `marginaleffects` package. With this function, we can specify exactly where we want to evaluate the comparison in the predictor space. Say we are interested in:

> The effect of changing passenger class on the predicted probability of survival for a 50 year old man and a 50 year old woman.

We can type:

```{r}
comparisons(mod,
  variables = "PClass", 
  newdata = datagrid(Age = 50, SexCode = 0:1))
```

Notice that the contrasts are different for the man and the woman. It appears that changing class has a larger effect on the expected probability of survival for men than for women. 

Note that when using `datagrid()` in this manner, variables that are not specified explicitly by the user are held at their mean or mode.


## Representative values

An alternative which used to be very common but has now fallen into a bit of disfavor is to compute "Contrasts at the mean." The idea is to create a "representative" or "synthetic" individual (row of the dataset) whose characteristics are completely average (or modal). Then, we compute and report the contrast for this specific hypothetical individual. For example:

```{r}
comparisons(mod, newdata = "mean")

comparisons(mod, newdata = "mean")
```

Contrasts at the mean can differ substantially from average contrasts.

The advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.


## Balanced grids

One type of grid which may be particularly useful in experimental setting is the "balanced grid", which includes all unique combinations of categorical variables, while holding numeric variables a their means. For example, in the Titanic dataset there are two categorical variables, `SexCode` and `PClass`, with 2 and 3 categories respectively. A balanced grid, will thus have rows. We can create this grid with:

```{r}
datagrid(model = mod, grid_type = "balanced")
```

When using `datagrid()` inside a `comparisons()` call, we do not need to specify the `model` argument manually:


```{r}
comparisons(mod,
    variables = "SexCode",
    newdata = datagrid(grid_type = "balanced"))
```


# Aggregation

As discussed above, the default behavior of `comparisons()` is to estimate quantities of interest for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an "average contrast". The procedure is as follows:

1. Compute estimates for each row of the original dataset.
2. Average the estimates across the whole dataset, or within subgroups.

To average estimates, we call the same function as before, with the previx `avg_`, and possibly with the `by` argument:

```{r}
avg_comparisons(mod)
```

This is equivalent to:

```{r}
cmp <- comparisons(mod)

mean(cmp$estimate[cmp$term == "Age"])

aggregate(estimate ~ term + contrast, data = cmp, FUN = mean)
```

## Subgroups

Using the `by` argument we can compute the "average effect" (i.e., "contrast" or "risk difference") of an increase of 10 years of `Age` in each of the passenger classes:

```{r}
avg_comparisons(mod,
    variables = list(Age = 10),
    by = "PClass")
```


## Marginal means

In experimental studies, it can often make sense to estimate contrasts, averaged across a balanced grid of treatment conditions:

```{r}
avg_comparisons(mod, newdata = datagrid(grid_type = "balanced"))
```

Note that the results are slightly different than when averaging across the empirical, because the number of actual passengers aboard the Titanic was not perfectly balanced across gender and ticket categories:

```{r}
avg_comparisons(mod)
```

# Uncertainty

By default, the standard errors around contrasts are computed using the delta method. This vignette offers extensive discussion and examples:

<https://marginaleffects.com/vignettes/uncertainty.html>

## Robust (clustered) standard errors

The standard errors reported by default for most models are "classical" (or "iid"). For models supported by the `sandwich` or `clubSandwich` packages, we can call on the `vcov` argument to report robust standard errors (and confidence intervales, p values, etc.):

Classical:

```{r}
avg_comparisons(mod, variables = "SexCode")
```

Heteroskedasticity-robust:

```{r}
avg_comparisons(mod, vcov = "HC3", variables = "SexCode")
```

## Bootstrap and simulation

We can use bootstrapping or simulations to compute uncertainty estimates, via the  `inferences()` function. Notice that, for some quantities, the classical intervals are very different (here: symmetric) than boostrap intervals (here: asymmetric):

```{r}
#| cache: true
#| warning: false
# classical
avg_comparisons(mod, comparison = "lnor", variables = "SexCode")

# bootstrap
avg_comparisons(mod, comparison = "lnor", variables = "SexCode") |>
    inferences("rsample")
```

## Transformation

By default, the standard errors around contrasts are computed on the scale determined by the `type` argument (e.g., "link" or "response"). Some analysts may prefer to proceed differently. For example, in `Stata`, the `adjrr` computes adjusted risk ratios (ARR) in two steps:

1.  Compute the natural log of the ratio between the mean of adjusted predictions with $x+1$ and the mean of adjusted predictions with $x$.
2.  Exponentiate the estimate and confidence interval bounds.

Step 1 is easy to achieve with the `comparison` argument described above. Step 2 can be achieved with the `transform` argument:

```{r}
avg_comparisons(
    mod,
    comparison = function(hi, lo) log(hi / lo),
    transform = exp)
```

Note that we can use the `lnratioavg` shortcut instead of defining the function ourselves.

The order of operations in previous command was:

1.  Compute the custom unit-level log ratios
2.  Exponentiate them
3.  Take the average using the `avg_comparisons()`

There is a very subtle difference between the procedure above and this code:

```{r}
avg_comparisons(
    mod,
    comparison = function(hi, lo) log(hi / lo),
    transform = exp)
```

Since the `exp` function is now passed to the `transform` argument of the `comparisons()` function, the exponentiation is now done only *after* unit-level contrasts have been averaged. This is what `Stata` appears to do under the hood, and the results are slightly different.

```{r}
comparisons(
    mod,
    comparison = function(hi, lo) log(mean(hi) / mean(lo)),
    transform = exp)
```

Note that equivalent results can be obtained using shortcut strings in the `comparison` argument: "ratio", "lnratio", "lnratioavg".

```{r}
comparisons(
    mod,
    comparison = "lnratioavg",
    transform = exp)
```


# Hypothesis

Now, let's see how we can conduct even more specific queries, using hypothesis tests. The `marginaleffects` website includes [a very detailed vignette on hypothesis tests.](https://marginaleffects.com/vignettes/hypothesis.html). It showcases very complex aggregations, comparisons, and tests. Here, we only introduce simple tests.

Imagine one is interested in this question:

> Does moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?

To answer this, we can start by using the `comparisons()` and the `datagrid()` functions:

```{r}
cmp <- comparisons(mod,
  variables = list(PClass = c("1st", "3rd")),
  newdata = datagrid(Age = 50, SexCode = 0:1))
```

At first glane, it looks like the estimated contrast is more negative for women (`r sprintf("%.3f", cmp$estimate[2])`) than for men (`r sprintf("%.3f", cmp$estimate[1])`). But is the difference between then statistical significant? To answer this question, we express the test of equality as a string formula, where  `b1` identifies the estimate in the first row and  `b2` the second row:

```{r}
comparisons(mod,
  hypothesis = "b1 = b2",
  variables = list(PClass = c("1st", "3rd")),
  newdata = datagrid(Age = 50, SexCode = 0:1))
```

The p value is small, which means we can reject the null hypothesis that the effect of moving from 1st to 3rd class on the probability of survival is the same for men and women.

The calculation above is equivalent to:

```{r}
cmp$estimate[1] - cmp$estimate[2]
```

We can also compare all the average contrasts in a call, by specifying `hypothesis="pairwise"`:

```{r}
avg_comparisons(mod, hypothesis = "pairwise")
```


# Plot

The `plot_comparisons()` function can plot comparisons, differences, risk ratios, etc. The [plotting vignette gives detailed insight into the use of this function.](https://marginaleffects.com/vignettes/plot.html) Here, it will be sufficient to illustrate its use by a few examples, and to note that most of the arguments of `comparisons()` are supported by `plot_comparisons()`.

Here, we plot Adjusted Risk Ratio and Adjusted Risk Difference in a model with a quadratic term:

```{r}
library(ggplot2)
library(patchwork)

mod_titanic <- glm(
    Survived ~ Sex * PClass + Age + I(Age^2),
    family = binomial,
    data = dat)

p1 <- plot_comparisons(
    mod_titanic,
    variables = "Age",
    condition = "Age",
    comparison = "ratio") +
    ylab("Adjusted Risk Ratio\nP(Survival | Age + 1) / P(Survival | Age)")

p2 <- plot_comparisons(
    mod_titanic,
    variables = "Age",
    condition = "Age",) +
    ylab("Adjusted Risk Difference\nP(Survival | Age + 1) - P(Survival | Age)")

p1 + p2
```



# Visual guide

This section illustrates some of the concepts in this vignette visually.

Consider a model with an interaction term. What happens to the dependent variable when the `hp` variable increases by 10 units?

```{r}
library(marginaleffects)

mt <- lm(mpg ~ hp * wt, data = mtcars)

plot_comparisons(
    mt,
    variables = list(hp = 10),
    condition = "wt")
```

```{r}
okabeito <- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')
options(ggplot2.discrete.fill = okabeito)
library(ggplot2)
theme_set(theme_minimal())

library(marginaleffects)
library(ggplot2)

set.seed(1024)
n <- 200
d <- data.frame(
  y = rnorm(n),
  cond = as.factor(sample(0:1, n, TRUE)),
  episode = as.factor(sample(0:4, n, TRUE)))

model1 <- lm(y ~ cond * episode, data = d)

p <- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))
ggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +
  geom_point()

# do episodes 1 and 2 differ when `cond=0`
ggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +
  geom_point() +
  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = "black") +
  ggtitle("What is the vertical distance between the linked points?")

comparisons(model1,
  variables = list(episode = 1:2), # comparison of interest
  newdata = datagrid(cond = 0))    # grid

# do cond=0 and cond=1 differ when episode = 1
ggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +
  geom_point() +
  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +
  ggtitle("What is the vertical distance between the linked points?")

comparisons(model1,
  variables = "cond",              # comparison of interest
  newdata = datagrid(episode = 1)) # grid

# Is the difference between episode 1 and 2 larger in cond=0 or cond=1? 
# try this without the `hypothesis` argument to see what we are comparing more clearly
ggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +
  geom_point() +
  annotate("rect", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = "green") +
  annotate("rect", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = "orange")  +
  ggtitle("Is the green box taller than the orange box?")

comparisons(model1,
  variables = list(episode = 1:2), # comparison of interest
  newdata = datagrid(cond = 0:1),  # grid
  hypothesis = "b1 = b2")          # hypothesis
```

# More

## Manual computation

Now we show how to use the base `R` `predict()` function to compute some of the same quantities as above. This exercise may be clarifying for some users.

```{r}
grid_50_1_3 <- data.frame(Age = 50, SexCode = 1, PClass = "3rd")
grid_50_1_1 <- data.frame(Age = 50, SexCode = 1, PClass = "1st")
grid_50_0_3 <- data.frame(Age = 50, SexCode = 0, PClass = "3rd")
grid_50_0_1 <- data.frame(Age = 50, SexCode = 0, PClass = "1st")


yhat_50_1_3 <- predict(mod, newdata = grid_50_1_3, type = "response")
yhat_50_1_1 <- predict(mod, newdata = grid_50_1_1, type = "response")
yhat_50_0_3 <- predict(mod, newdata = grid_50_0_3, type = "response")
yhat_50_0_1 <- predict(mod, newdata = grid_50_0_1, type = "response")

## prediction on a grid
predictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = "3rd"))
yhat_50_1_3

## contrast on a grid
comparisons(mod,
  variables = list(PClass = c("1st", "3rd")),
  newdata = datagrid(Age = 50, SexCode = 0:1))

yhat_50_0_3 - yhat_50_0_1
yhat_50_1_3 - yhat_50_1_1

## difference-in-differences 
comparisons(mod,
  variables = list(PClass = c("1st", "3rd")),
  newdata = datagrid(Age = 50, SexCode = 0:1),
  hypothesis = "b1 = b2")

(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)

## average of the empirical distribution of contrasts
avg_comparisons(mod, variables = list(PClass = c("1st", "3rd")), by = "SexCode")

grid_empirical_1_3 <- dat |> subset(SexCode == 1) |> transform(PClass = "3rd")
grid_empirical_1_1 <- dat |> subset(SexCode == 1) |> transform(PClass = "1st")
grid_empirical_0_3 <- dat |> subset(SexCode == 0) |> transform(PClass = "3rd")
grid_empirical_0_1 <- dat |> subset(SexCode == 0) |> transform(PClass = "1st")
yhat_empirical_0_1 <- predict(mod, newdata = grid_empirical_0_1, type = "response")
yhat_empirical_0_3 <- predict(mod, newdata = grid_empirical_0_3, type = "response")
yhat_empirical_1_1 <- predict(mod, newdata = grid_empirical_1_1, type = "response")
yhat_empirical_1_3 <- predict(mod, newdata = grid_empirical_1_3, type = "response")
mean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)
mean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)
```

## Transformations

So far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:

-   `comparison`
-   `transform`

Consider the case of a model with a single predictor $x$. To compute average contrasts, we proceed as follows:

1.  Compute adjusted predictions for each row of the dataset for the observed values $x$: $\hat{y}_x$
2.  Compute adjusted predictions for each row of the dataset for the observed values $x + 1$: $\hat{y}_{x+1}$
3.  `comparison`: Compute unit-level contrasts by taking the difference between (or some other function of) adjusted predictions: $\hat{y}_{x+1} - \hat{y}_x$
4.  Compute the average contrast by taking the mean of unit-level contrasts: $1/N \sum_{i=1}^N \hat{y}_{x+1} - \hat{y}_x$
5.  `transform`: Transform the average contrast or return them as-is.

The `comparison` argument of the `comparisons()` function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with `hi` value of $x$, and predictions with a `lo` value of $x$: `function(hi, lo) hi-lo`.

The `transform` argument of the `comparisons()` function applies a custom transformation to the unit-level contrasts.

The `transform` argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without `transform`.


This call adds the `transform` argument to exponentiate the log-odds ratios at the very end of calculations:

```{r}
mod <- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)

comparisons(mod, 
    comparison = "lnor", 
    transform = exp,
    newdata = passenger)
```


## Difference-in-Differences

One thing we can notice in the Titanic example, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. 

```{r}
comparisons(
  mod,
  variables = "SexCode",
  newdata = datagrid(PClass = c("1st", "3rd")))
```

Indeed, being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?

To answer this question, we can compute a difference-in-difference using the `hypothesis` argument ([see the Hypothesis vignette for details](hypothesis.html)). For example, using `b1` and `b2` to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:

```{r}
comparisons(
  mod,
  hypothesis = "b1 - b2 = 0",
  variables = "SexCode",
  newdata = datagrid(PClass = c("1st", "3rd")))
```

Now, let's say we consider more types of individuals:

```{r}
comparisons(
  mod,
  variables = "SexCode",
  newdata = datagrid(PClass = c("1st", "3rd"), Age = range))
```

With these results, we could compute a triple difference:

```{r}
comparisons(
  mod,
  hypothesis = "(b1 - b3) - (b2 - b4) = 0",
  variables = "SexCode",
  newdata = datagrid(PClass = c("1st", "3rd"), Age = range))
```




## Forward, Backward, Centered

By default, the `comparisons()` function computes a "forward" difference. For example, if we ask `comparisons()` to estimate the effect of a 10-unit change in predictor `x` on outcome `y`, `comparisons()` will compare the predicted values with `x` and `x+10`.

```{r}
mt <- mtcars
mt$new_hp <- 49 * (mt$hp - min(mt$hp)) / (max(mt$hp) - min(mt$hp)) + 1
mod_mt <- lm(mpg ~ log(new_hp), data = mt)

avg_comparisons(
  mod_mt,
  variables = list(new_hp = 10))
```

We can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:

```{r}
forward_diff <- \(x) data.frame(x, x + 10)
backward_diff <- \(x) data.frame(x - 10, x)
center_diff <- \(x) data.frame(x - 5, x + 5)

avg_comparisons(
  mod_mt,
  variables = list(new_hp = forward_diff))

avg_comparisons(
  mod_mt,
  variables = list(new_hp = backward_diff))

avg_comparisons(
  mod_mt,
  variables = list(new_hp = center_diff))
```

Notice that the last "centered" difference gives the same results as the default `comparisons()` call.


## Observation-Wise Categorical Marginal Effect

For categorical predictors, [Scholbeck et al. 2023](https://arxiv.org/pdf/2201.08837.pdf) recommend that analysts report what they call the "observation-wise categorical marginal effects." They describe the procedure as follows:

> Recall that the common definition of categorical MEs is based on first changing all observations' value of $x_j$ to each category and then computing the difference in predictions when changing it to the reference category. However, one is often interested in prediction changes if aspects of an actual observation change. We therefore propose an observation-wise categorical ME. We first select a single reference category $c_h$. For each observation whose feature value $x_j \neq c_h$, we predict once with the observed value $x_j$ and once where $x_j$ has been replaced by $c_h$.

To achieve this with `marginaleffects`, we proceed in three simple steps:

1.  Use the `factor()` function to set the reference level of the categorical variable.
2.  Use the `newdata` argument to take the subset of data where the observed $x_j$ is different from the reference level we picked in 1.
3.  Apply the `avg_comparisons()` with the `"revreference"` option.

```{r}
mt <- transform(mtcars, cyl = factor(cyl, levels = c(6, 4, 8)))

mod_mt <- glm(vs ~ mpg * factor(cyl), data = mt, family = binomial)

avg_comparisons(mod_mt,
  variables = list(cyl = "revreference"),
  newdata = subset(mt, cyl != 6))
```






````{comment}
# Marginal means

Yet another type of contrast is the "Contrast between marginal means." This type of contrast is closely related to the "Contrast at the mean", with a few wrinkles. It is the default approach used by the `emmeans` package for `R`.

Roughly speaking, the procedure is as follows: 

1. Create a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.
2. Make adjusted predictions in each cell of the prediction grid.
3. Take the average of those predictions (marginal means) for each combination of `btype` (focal variable) and `resp` (group `by` variable).
4. Compute pairwise differences (contrasts) in marginal means across different levels of the focal variable `btype`.

The contrast obtained through this approach has two critical characteristics:

a) It is the contrast for a synthetic individual with perfectly average qualities on every (numeric) predictor.
b) It is a weighted average of unit-level contrasts, where weights assume a perfectly balanced dataset across every categorical predictor.

With respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted *as if* the data were perfectly balanced? 

For example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.

If the contrasts between marginal means is really the quantity of interest, it is easy to use the `comparisons()` to estimate contrasts between marginal means. The `newdata` determines the values of the predictors at which we want to compute contrasts. We can set `newdata="marginalmeans"` to emulate the `emmeans` behavior. For example, here we compute contrasts in a model with an interaction:

```{r}
dat <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv")
mod <- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)

avg_comparisons(
    mod,
    newdata = "marginalmeans",
    variables = c("species", "island"))
```

Which is equivalent to this in `emmeans`:

```{r}
library(emmeans)
emm <- emmeans(
    mod,
    specs = c("species", "island"))
contrast(emm, method = "trt.vs.ctrl1")
```

The [`emmeans` section of the Alternative Software vignette](alternative_software.html#emmeans) shows further examples.

The [excellent vignette of the `emmeans` package](https://CRAN.R-project.org/package=emmeans/vignettes/basics.html) discuss the same issues in a slightly different (and more positive) way:

> The point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.

> All this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace.
````


